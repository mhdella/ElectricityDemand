{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.imputation.mice as smi\n",
    "import copy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load with various verbose summaries\n",
    "def load_csv(file_path, columns, na_vals=['NA',], parse_d=True, verbose=0):\n",
    "    dtype_map = {}\n",
    "    for col in columns:\n",
    "        dtype_map[col] = np.float64\n",
    "    df = pd.read_csv(file_path,\n",
    "                    dtype=dtype_map,\n",
    "                    parse_dates=parse_d,\n",
    "                    na_values=na_vals)\n",
    "    if verbose >= 1:\n",
    "        print(\"\\nHead(10)\")\n",
    "        print(df.head(10))\n",
    "        print(\"\\nDescribe\")\n",
    "        print(df.describe().round(2))\n",
    "        print(\"\\ndf.isna().sum(axis=0)\")\n",
    "        print(df.isna().sum(axis=0))\n",
    "    if verbose >= 2:\n",
    "        plt.imshow(~df.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.show()\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lags(df, regions):\n",
    "    for r in regions:\n",
    "        print(\"Adding lag for region {}\".format(r))\n",
    "        kwargs = {\n",
    "                 '{}_Lag1'.format(r) : lambda x: np.roll(df[r], +1),\n",
    "             }\n",
    "        df = df.assign(**kwargs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_col(df, col):\n",
    "    return df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "# Impute and save the imputed datasets\n",
    "# Not sure what the burn in is for these\n",
    "def impute_MICEData(df, regions, n_iters):\n",
    "    imp = smi.MICEData(df)\n",
    "    regs = list(regions)\n",
    "    r_main = regs.pop()\n",
    "    regs2 = []\n",
    "    for r in regs:\n",
    "        regs2.append(r)\n",
    "        regs2.append(r+'_Lag1')\n",
    "    regs2.append(r_main+'_Lag1')\n",
    "    f = ' + '.join(regs2)\n",
    "    imp.set_imputer(r_main, formula=f)\n",
    "    for j in range(n_iters):\n",
    "        print(\"MICE imputation {}\".format(j))\n",
    "        imp.update_all()\n",
    "        imp.data.to_csv('data%02d.csv' % j, index=False)\n",
    "\n",
    "file_path = 'pre_imputed_CA_data.csv'\n",
    "regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "df = load_csv(file_path, regions, ['NA',], True, 2)\n",
    "#df = add_lags(df, regions)\n",
    "#df = drop_col(df, 'date_time')\n",
    "#impute_MICEData(df, regions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_imputed_indices(raw, name):\n",
    "    index = pd.isnull(raw[name]).nonzero()[0]\n",
    "    return index\n",
    "\n",
    "def get_overimpute_index(raw, imp, col):\n",
    "    init_nan = return_imputed_indices(raw, col)\n",
    "    init_nan_set = set()\n",
    "    for i in init_nan:\n",
    "        init_nan_set.add(i)\n",
    "    \n",
    "    over_nan = return_imputed_indices(imp, col)\n",
    "    over_nan_set = set()\n",
    "    for i in over_nan:\n",
    "        over_nan_set.add(i)\n",
    "    \n",
    "    return np.array(list(over_nan_set.difference(init_nan_set)))\n",
    "\n",
    "def split_index_into_sort_and_long_gaps(index):\n",
    "    short = []\n",
    "    long = []\n",
    "    index.sort()\n",
    "    prev_was_short = False\n",
    "    for i in range(len(index)-2): # Can't compare the last one like this\n",
    "        if index[i+1] == index[i] + 1 and index[i+2] == index[i] + 2:\n",
    "            long.append(index[i])\n",
    "            prev_was_short = False\n",
    "        else:\n",
    "            short.append(index[i])\n",
    "            prev_was_short = True\n",
    "    if prev_was_short:\n",
    "        short.append(index[-2])\n",
    "        short.append(index[-1])\n",
    "    else:\n",
    "        long.append(index[-2])\n",
    "        long.append(index[-1])\n",
    "    return short, long\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "def return_values_by_index(imp, indices, name, replace_nan_with_zero=True):\n",
    "    vals = imp.loc[indices, name]\n",
    "    if replace_nan_with_zero:\n",
    "        vals = vals.fillna(0)\n",
    "    return vals\n",
    "\n",
    "def comparison_demand_plot(original, imp, imp_names, title, save):\n",
    "    \n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('Demand')\n",
    "    plt.title(title)\n",
    "    ax.plot(original['CISO'], '-', label='CISO Raw')\n",
    "    for v, name in zip(imp, imp_names):\n",
    "        ax.plot(v, 'o', label=name)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "# Create many demand plots so we can actually see the values\n",
    "def scrolling_demand(width, original, imp, imp_names, title, save):\n",
    "    start = 0\n",
    "    end = width\n",
    "    i = 0\n",
    "    tot_l = len(original.index)\n",
    "    while True:\n",
    "        s = save.replace('.png', '_{}cnt'.format(i))\n",
    "        t = title+': cnt {}'.format(i)\n",
    "        o = original.loc[start:end]\n",
    "        imps = []\n",
    "        empty = True\n",
    "        for im in imp:\n",
    "            imps.append(im.loc[start:end])\n",
    "            if len(imps[-1].index) > 0:\n",
    "                empty = False\n",
    "        if not empty:\n",
    "            comparison_demand_plot(o, imps, imp_names, t, s)\n",
    "        if end == tot_l:\n",
    "            break\n",
    "        i += 1\n",
    "        start += width\n",
    "        end += width\n",
    "        if end >= tot_l:\n",
    "            end = tot_l\n",
    "\n",
    "\n",
    "def comparison_scatter_plot(v1s, v2s, labels, t1, t2, title, save, float_y_min=False):\n",
    "\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.set_xlabel(t1)\n",
    "    ax.set_ylabel(t2)\n",
    "    max_v1s = np.max( list(map(lambda x: np.max(x), v1s)))\n",
    "    max_v2s = np.max( list(map(lambda x: np.max(x), v2s)))\n",
    "    ax.set_xlim(0, max_v1s*1.1)\n",
    "    min_v2s = 0\n",
    "    if float_y_min:\n",
    "        min_v2s = np.min( list(map(lambda x: np.min(x), v2s)))\n",
    "    ax.set_ylim(min_v2s, max_v2s*1.1)\n",
    "    plt.title(title)\n",
    "    for v1, v2, l in zip(v1s, v2s, labels):\n",
    "        ax.plot(v1, v2, '.', label=l)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "def simple_resolution(df_true, df_imp, title, save, n_bins=20):\n",
    "\n",
    "    plt.close()\n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting simple_resolution\")\n",
    "    #    return 0\n",
    "\n",
    "    #res_grid = []\n",
    "    #for index, value in df_true.items():\n",
    "    #    if value > 0:\n",
    "    #        res_grid.append( (df_imp.at[index]-value) / value)\n",
    "    #    else:\n",
    "    #        print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    res_grid = []\n",
    "    for obs, val in zip(df_true, df_imp):\n",
    "        if obs != 0:\n",
    "            res_grid.append((val-obs)/obs)\n",
    "        else:\n",
    "            print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    n, bins, patches = ax.hist(res_grid, n_bins, \n",
    "            facecolor='b', alpha=0.5, density=False)\n",
    "    print(\"Length simp_res {}\".format(np.sum(n)))\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "# df_master is used to get the correct UCT time\n",
    "def resolution_by_time(df_master, idx_vals, df_true, df_imp, title, save):\n",
    "\n",
    "    plt.close()\n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting resolution_by_time\")\n",
    "    #    return 0\n",
    "    \n",
    "    # Get UCT time by index\n",
    "    zero_index_hour = datetime.datetime.strptime(df_master.at[0, 'date_time'], \"%Y-%m-%dT%H:%M:%SZ\").hour\n",
    "    \n",
    "    res_grid = []\n",
    "    for i in range(24):\n",
    "        res_grid.append([])\n",
    "    month_grid = []\n",
    "    for i in range(12):\n",
    "        month_grid.append([])\n",
    "    \n",
    "    for idx, obs, val in zip(idx_vals, df_true, df_imp):\n",
    "        mod = (idx + zero_index_hour)%24\n",
    "        res = (val-obs)/obs\n",
    "        res_grid[mod].append(res)\n",
    "        # Get month\n",
    "        month = datetime.datetime.strptime(df_master.at[idx, 'date_time'], \"%Y-%m-%dT%H:%M:%SZ\").month\n",
    "        month_grid[month-1].append(res)\n",
    "\n",
    "    # Plot hourly\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.set_title(title+': whiskers at 5%/95%')\n",
    "    ax.boxplot(res_grid, whis=[5, 95])\n",
    "    ax.set_xlabel('Hour (UCT)')\n",
    "    ax.set_ylabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    # Plot monthly\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.set_title(title.replace('hour', 'month')+': whiskers at 5%/95%')\n",
    "    ax.boxplot(month_grid, whis=[5, 95])\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save.replace('hour', 'month'))\n",
    "    \n",
    "    \n",
    "    \n",
    "width = 500\n",
    "n1 = 'MICE'\n",
    "## Open a saved csv and check contents\n",
    "base = '/Users/truggles/Downloads/'\n",
    "imp_map = { # input impute file: returned imputed file,\n",
    "    'ca_for_overimpute3.csv' : 'mean_impute_CA_overimpute3_mice (2).csv',\n",
    "    'ca_for_overimpute3_1.csv' : 'mean_impute_CA_overimpute3_1_mice.csv',\n",
    "    'ca_for_overimpute3_2.csv' : 'mean_impute_CA_overimpute3_2_mice.csv',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "regs = ['TIDC',]\n",
    "# Loop all regions\n",
    "for r in regions:\n",
    "    print(r)\n",
    "    record = {\n",
    "        'idx_all' : [],\n",
    "        'idx_short' : [],\n",
    "        'idx_long' : [],\n",
    "        \n",
    "        'obs_all' : [],\n",
    "        'obs_short' : [],\n",
    "        'obs_long' : [],\n",
    "\n",
    "        'imp_all' : [],\n",
    "        'imp_short' : [],\n",
    "        'imp_long' : [],\n",
    "    }\n",
    "    # Loop all imputed files\n",
    "    for k, v in imp_map.items():\n",
    "        df_imp = load_csv(base+k, regions, ['NA',], True, 0)\n",
    "        df_mice = load_csv(base+v, regions, ['NA',], True, 0)\n",
    "        \n",
    "        #indices = return_imputed_indices(df, r)\n",
    "\n",
    "        # All imputed points comparing algos against eachother\n",
    "        #v1 = return_values_by_index(df_mice, indices, r)\n",
    "\n",
    "        #comparison_demand_plot(df, [v1,], [n1,],\n",
    "        #    'Imputation of {}'.format(r), 'imp_dem_{}_comp.png'.format(r))\n",
    "        #scrolling_demand(width, df, [v1,], [n1,],\n",
    "        #    'Imputation of {}'.format(r), '/Users/truggles/tmp_plots/imp_dem_{}_comp.png'.format(r))\n",
    "\n",
    "        \n",
    "        # Find index ONLY from overimputation\n",
    "        over_index = get_overimpute_index(df, df_imp, r)\n",
    "\n",
    "        # Split results by short vs long imputation gaps\n",
    "        short_obs, long_obs = split_index_into_sort_and_long_gaps(over_index)\n",
    "        record['idx_all'].append(over_index)\n",
    "        record['idx_short'].append(short_obs)\n",
    "        record['idx_long'].append(long_obs)\n",
    "        \n",
    "        record['obs_all'].append(return_values_by_index(df, over_index, r))\n",
    "        record['obs_short'].append(return_values_by_index(df, short_obs, r))\n",
    "        record['obs_long'].append(return_values_by_index(df, long_obs, r))\n",
    "        \n",
    "        record['imp_all'].append(return_values_by_index(df_mice, over_index, r))\n",
    "        record['imp_short'].append(return_values_by_index(df_mice, short_obs, r))\n",
    "        record['imp_long'].append(return_values_by_index(df_mice, long_obs, r))\n",
    "    \n",
    "    idxs = ['idx_all', 'idx_short', 'idx_long']\n",
    "    obss = ['obs_all', 'obs_short', 'obs_long']\n",
    "    imps = ['imp_all', 'imp_short', 'imp_long']\n",
    "    names = ['all', 'short', 'long']\n",
    "    for name, idx, obs, imp in zip(names, idxs, obss, imps):\n",
    "        idx_vals = np.concatenate(record[idx])\n",
    "        obs_vals = pd.concat(record[obs])\n",
    "        imp_vals = pd.concat(record[imp])\n",
    "        \n",
    "        simple_resolution(obs_vals, imp_vals, 'overimp resolution {} {} {}'.format(name, n1, r),\n",
    "            'impOver_resolution_{}_{}_{}.png'.format(r, n1, name), 30)\n",
    "        resolution_by_time(df, idx_vals, obs_vals, imp_vals, 'overimp hourly resolution {} {} {}'.format(name, n1, r),\n",
    "            'imp_resolution_hourly_{}_{}_{}.png'.format(r, n1, name))\n",
    "        comparison_scatter_plot([obs_vals.values,],\n",
    "            [imp_vals.values,], [n1+':Obs',], 'Observed', 'Imputed',\n",
    "            'Comparing {} for region: {}'.format(n1, name), 'imp_scatter_overImp_{}_{}_comp.png'.format(r, name))\n",
    "        # Scatter with resolution on y-axis\n",
    "        res1 = []\n",
    "        for val, obs in zip(imp_vals, obs_vals):\n",
    "            res1.append((val-obs)/obs)\n",
    "        float_y_min = True\n",
    "        comparison_scatter_plot([obs_vals.values,],\n",
    "            [res1,], [n1+' Resolution',], 'Observed', '(Imp. - Obs.)/Obs.',\n",
    "            'Resolution {} for region: {}'.format(n1, name), 'imp_scatter_overImp_res_{}_{}_comp.png'.format(r, name), float_y_min)       \n",
    "\n",
    "#for k1, v1 in ordered.items():\n",
    "#    for k2, v2 in v1.items():\n",
    "#        print(k1, k2, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_for_missing_structure(df):\n",
    "    rec = {}\n",
    "    for col in df.columns:\n",
    "        if 'Lag1' in col or 'date_time' in col: continue\n",
    "        rec[col] = [0, []] # missing tally, and record\n",
    "    for index, row in df.iterrows():\n",
    "        for col, info in rec.items():\n",
    "            if np.isnan(row[col]): # incriment missing tally\n",
    "                info[0] += 1\n",
    "            elif not np.isnan(row[col]) and info[0] > 0:\n",
    "                info[1].append(info[0])\n",
    "                info[0] = 0\n",
    "    #for k, v in rec.items():\n",
    "    #    print(k, v)\n",
    "    return rec\n",
    "\n",
    "# Not used\n",
    "def group_results(groupings, results):\n",
    "    cats = {}\n",
    "    for g in groupings:\n",
    "        cats[g] = 0\n",
    "    res = {}\n",
    "    for col, info in results.items():\n",
    "        res[col] = copy.deepcopy(cats)\n",
    "        for v in info[1]:\n",
    "            if v == 1: res[col]['1'] += 1\n",
    "            elif v <= 5: res[col]['2-5'] += 1\n",
    "            elif v <= 23: res[col]['5-23'] += 1\n",
    "            elif v <= 168: res[col]['24-168'] += 1\n",
    "            elif v <= 1000: res[col]['169-1000'] += 1\n",
    "            elif v <= 2000: res[col]['1001-2000'] += 1\n",
    "            elif v <= 3000: res[col]['2001-3000'] += 1\n",
    "            else: res[col]['3001+'] += 1\n",
    "    for k, v in res.items():\n",
    "        print(k, v)\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_locations(df, requested_gaps, cnt=-1):\n",
    "    # Begin with the longest requested gaps and work you way\n",
    "    # to smaller requested gaps\n",
    "    for col, info in requested_gaps.items():\n",
    "        info[1].sort()\n",
    "        info[1].reverse()\n",
    "        #print(col, info[1])\n",
    "        for length in info[1]:\n",
    "            # Try in requested column first the simple way, if that doesn't\n",
    "            # work, try the \"difficult\" way where some existing\n",
    "            # np.nans will be included in naned data\n",
    "            if find_and_remove_location(df, col, length):\n",
    "                find_and_remove_difficult_location(df, col, length, cnt)\n",
    "\n",
    "\n",
    "\n",
    "# Loop over vals in the dataframe and find a continous region which\n",
    "# has a reasonably \"good\" data buffer around the requested length of data to remove.\n",
    "# Change the valse to np.nan\n",
    "def find_and_remove_location(df, col, length, verbose=False):\n",
    "    # Start at a random index position to not bias the removals\n",
    "    # all towards the front\n",
    "    max_good_data = 0\n",
    "    loc = int(np.random.uniform(0, len(df.index)))\n",
    "    if loc == len(df.index):\n",
    "        loc -= 1\n",
    "    start_of_good_data = loc\n",
    "    length_of_good_data = 0\n",
    "    n_loops = 0\n",
    "    # How much good data on each side of the new gap?\n",
    "    if length <= 100:\n",
    "        tgt_length = 5 * length \n",
    "        buffer = 2 * length # 2x on each side\n",
    "    elif length <= 1000:\n",
    "        tgt_length = 3 * length \n",
    "        buffer = 1 * length # 1x on each side\n",
    "    else:\n",
    "        tgt_length = int(1.5 * length) \n",
    "        buffer = int(0.25 * length) # 0.25x on each side\n",
    "    while True:\n",
    "        if np.isnan(df.at[loc, col]):\n",
    "            start_of_good_data = loc + 1 # This is the following value\n",
    "            # and will continuously incriment if isnan()\n",
    "            length_of_good_data = 0\n",
    "        else: # good data\n",
    "            length_of_good_data += 1\n",
    "            if length_of_good_data > max_good_data:\n",
    "                max_good_data = length_of_good_data\n",
    "\n",
    "        # Remember pandas DataFrame has different slice notation that normal python\n",
    "        # where the terminal value is included in the slice\n",
    "        if length_of_good_data == tgt_length - 1:\n",
    "            if verbose:\n",
    "                print(\"Found a great spot for removal, col {:}, l={:d}, tgt_l={:d}, buffer={:d}, [{:d}:{:d}]\".format(\n",
    "                        col, length, tgt_length, buffer, start_of_good_data, start_of_good_data+length_of_good_data))\n",
    "            strt = start_of_good_data + buffer # Begin nan after good data buffer \n",
    "            end = start_of_good_data + buffer + length - 1\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            df.loc[strt:end, col] = np.nan\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            return 0\n",
    "        loc += 1\n",
    "        \n",
    "        # Wrap to start of df\n",
    "        if loc >= len(df.index):\n",
    "            loc = 0\n",
    "            n_loops += 1\n",
    "            if n_loops > 1:\n",
    "                print(\"Too many loops for col {} and requested length {}, max good data length {}\".format(\n",
    "                    col, length, max_good_data))\n",
    "                return 1\n",
    "\n",
    "            \n",
    "def find_and_remove_difficult_location(df, col, length, to_take=-1, verbose=False):\n",
    "    # Scan data and look for highest purity \"good\" data region\n",
    "    # to apply np.nan\n",
    "    rec = []\n",
    "    print(\"find_and_remove_difficult_location\")\n",
    "    print(\" - Looking for col {} for length {}\".format(col, length))\n",
    "    for i in range(0, int(len(df.index) - 1.5 * length)):\n",
    "        rec.append((i, df.loc[i:int(i+1.5*length), col].isna().sum()))\n",
    "    nan_min = 9999\n",
    "    best_idx = -1\n",
    "    for val in rec:\n",
    "        if val[1] < nan_min:\n",
    "            nan_min = val[1]\n",
    "            best_idx = val[0]\n",
    "\n",
    "    # Find other comparable locations for adding a gap.\n",
    "    # Search for reginons with 20% more less NANs\n",
    "    # with respect to length requested.\n",
    "    others = []\n",
    "    for val in rec:\n",
    "        if val[1] <= length * 0.2 and abs(val[0]-best_idx) > length:\n",
    "            new_gap = True\n",
    "            for other in others:\n",
    "                if abs(val[0]-other[0]) < length:\n",
    "                    new_gap = False\n",
    "            if new_gap and val[0] - length < len(df.index):\n",
    "                others.append(val)\n",
    "    print(\" - others:\")\n",
    "    print(others)\n",
    "    \n",
    "    \n",
    "    print(\" - Best idx {} for nan count of {}\".format(best_idx, nan_min))\n",
    "    if to_take >= 0:\n",
    "        print(others[to_take][0])\n",
    "        print(\" - Will select location based on 'Others' {} {}\".format(to_take, others[to_take]))\n",
    "        df.loc[int(others[to_take][0]+0.25*length):int(others[to_take][0]+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in 'other' location {} with {} total np.nans\".format(\n",
    "            others[to_take][0],\n",
    "            df.loc[others[to_take][0]:int(others[to_take][0]+1.5*length), col].isna().sum()))\n",
    "    else:\n",
    "        df.loc[int(best_idx+0.25*length):int(best_idx+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in defaul location {} with {} total np.nans\".format(\n",
    "            best_idx,\n",
    "            df.loc[best_idx:int(best_idx+1.5*length), col].isna().sum()))\n",
    "            \n",
    "\n",
    "np.random.seed(1)\n",
    "results = scan_for_missing_structure(df)\n",
    "#groupings = ['1', '2-5', '5-23',  '24-168', '169-1000', '1001-2000',\n",
    "#            '2001-3000', '3001+']\n",
    "plt.imshow(~df.isna(), aspect='auto')\n",
    "plt.xlabel(\"variables\")\n",
    "plt.ylabel(\"cases\")\n",
    "plt.gray()\n",
    "plt.show()\n",
    "\n",
    "make_overimpute = False\n",
    "if make_overimpute:\n",
    "    for i in range(-1, 9): # Length of 'others' in difficult replace code for 4 CA regions\n",
    "        df2 = copy.deepcopy(df)\n",
    "        results2 = copy.deepcopy(results)\n",
    "        remove_locations(df2, results2, i)\n",
    "        plt.imshow(~df2.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.show()\n",
    "        print(\"Saving as 'ca_for_overimpute3_{}.csv'\".format(str(i).replace('-1','default')))\n",
    "        df2.to_csv('ca_for_overimpute3_{}.csv'.format(str(i).replace('-1','default')), index=False, na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
