{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.imputation.mice as smi\n",
    "import copy\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load with various verbose summaries\n",
    "def load_csv(file_path, columns, na_vals=['NA',], parse_d=True, verbose=0):\n",
    "    dtype_map = {}\n",
    "    for col in columns:\n",
    "        dtype_map[col] = np.float64\n",
    "    df = pd.read_csv(file_path,\n",
    "                    dtype=dtype_map,\n",
    "                    parse_dates=parse_d,\n",
    "                    na_values=na_vals)\n",
    "    if verbose >= 1:\n",
    "        print(\"\\nHead(10)\")\n",
    "        print(df.head(10))\n",
    "        print(\"\\nDescribe\")\n",
    "        print(df.describe().round(2))\n",
    "        print(\"\\ndf.isna().sum(axis=0)\")\n",
    "        print(df.isna().sum(axis=0))\n",
    "    if verbose >= 2:\n",
    "        plt.imshow(~df.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.show()\n",
    "    if 'date_time' in df.columns:\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lags(df, regions):\n",
    "    for r in regions:\n",
    "        print(\"Adding lag for region {}\".format(r))\n",
    "        kwargs = {\n",
    "                 '{}_Lag1'.format(r) : lambda x: np.roll(df[r], +1),\n",
    "             }\n",
    "        df = df.assign(**kwargs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_col(df, col):\n",
    "    return df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "# Impute and save the imputed datasets\n",
    "# Not sure what the burn in is for these\n",
    "def impute_MICEData(df, regions, n_iters):\n",
    "    imp = smi.MICEData(df)\n",
    "    regs = list(regions)\n",
    "    r_main = regs.pop()\n",
    "    regs2 = []\n",
    "    for r in regs:\n",
    "        regs2.append(r)\n",
    "        regs2.append(r+'_Lag1')\n",
    "    regs2.append(r_main+'_Lag1')\n",
    "    f = ' + '.join(regs2)\n",
    "    imp.set_imputer(r_main, formula=f)\n",
    "    for j in range(n_iters):\n",
    "        print(\"MICE imputation {}\".format(j))\n",
    "        imp.update_all()\n",
    "        imp.data.to_csv('data%02d.csv' % j, index=False)\n",
    "\n",
    "file_path = 'pre_imputed_CA_data.csv'\n",
    "regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "df = load_csv(file_path, regions, ['NA',], True, 2)\n",
    "#df = add_lags(df, regions)\n",
    "#df = drop_col(df, 'date_time')\n",
    "#impute_MICEData(df, regions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_imputed_indices(raw, name):\n",
    "    index = pd.isnull(raw[name]).nonzero()[0]\n",
    "    return index\n",
    "\n",
    "def get_overimpute_index(raw, imp, col):\n",
    "    init_nan = return_imputed_indices(raw, col)\n",
    "    init_nan_set = set()\n",
    "    for i in init_nan:\n",
    "        init_nan_set.add(i)\n",
    "    \n",
    "    over_nan = return_imputed_indices(imp, col)\n",
    "    over_nan_set = set()\n",
    "    for i in over_nan:\n",
    "        over_nan_set.add(i)\n",
    "    \n",
    "    return np.array(list(over_nan_set.difference(init_nan_set)))\n",
    "\n",
    "def split_index_into_sort_and_long_gaps(index):\n",
    "    short = []\n",
    "    long = []\n",
    "    index.sort()\n",
    "    prev_was_short = False\n",
    "    for i in range(len(index)-2): # Can't compare the last one like this\n",
    "        if index[i+1] == index[i] + 1 and index[i+2] == index[i] + 2:\n",
    "            long.append(index[i])\n",
    "            prev_was_short = False\n",
    "        else:\n",
    "            short.append(index[i])\n",
    "            prev_was_short = True\n",
    "    if prev_was_short:\n",
    "        short.append(index[-2])\n",
    "        short.append(index[-1])\n",
    "    else:\n",
    "        long.append(index[-2])\n",
    "        long.append(index[-1])\n",
    "    return short, long\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "def return_values_by_index(imp, indices, name, replace_nan_with_zero=True):\n",
    "    vals = imp.loc[indices, name]\n",
    "    if replace_nan_with_zero:\n",
    "        vals = vals.fillna(0)\n",
    "    return vals\n",
    "\n",
    "def comparison_demand_plot(region, original, imp, imp_up, imp_down, imp_name, title, save, o_max):\n",
    "    \n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('Demand')\n",
    "    plt.title(title)\n",
    "    ax.plot(original['date_time'], original[region], 'b-', label='{} True'.format(region))\n",
    "    ax.plot(original['date_time'], imp, 'r-', label='{} {} Impute'.format(region, imp_name))\n",
    "    ax.fill_between(original['date_time'], imp_down, imp_up, facecolor='orange', alpha=0.5)\n",
    "    ax.plot(original['date_time'], original[region], 'b-', label='_nolegend_')\n",
    "    ax.set_ylim(0, o_max*1.3)\n",
    "    plt.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    plt.tight_layout()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "# Create many demand plots so we can actually see the values\n",
    "def scrolling_demand(width, region, original, imps, imp_name, title, save):\n",
    "    start = 0\n",
    "    end = width-1\n",
    "    k = 0\n",
    "    tot_l = len(original.index)\n",
    "    o_max = np.nanmax(original[region])\n",
    "    while True:\n",
    "        print(start, end)\n",
    "        s = save.replace('.png', '_{}cnt'.format(k))\n",
    "        t = title+': cnt {}'.format(k)\n",
    "        o = original.loc[start:end]\n",
    "        imp_avg = copy.deepcopy(imps[0][region].loc[start:end])\n",
    "        imp_count = imps[0][region].loc[start:end].notna().astype(int)\n",
    "        for i, imp in enumerate(imps):\n",
    "            if i == 0:\n",
    "                continue # already initialized with [0]\n",
    "            imp_avg += imp[region].loc[start:end]\n",
    "            imp_count += imp[region].loc[start:end].notna().astype(int)\n",
    "        imp_avg = imp_avg / imp_count\n",
    "        \n",
    "        # Max and min\n",
    "        imp_max = []\n",
    "        imp_min = []\n",
    "        end_l = end if end == len(original.index) else end+1\n",
    "        for j in range(start, end_l):\n",
    "            # Could check if imputed, skip that time saver for now\n",
    "            max_ = -999\n",
    "            min_ = 999999\n",
    "            for imp in imps:\n",
    "                if imp.at[j, region] > max_:\n",
    "                    max_ = imp.at[j, region]\n",
    "                if imp.at[j, region] < min_:\n",
    "                    min_ = imp.at[j, region]\n",
    "            imp_max.append(max_)\n",
    "            imp_min.append(min_)\n",
    "        #print(len(o))\n",
    "        #print(len(imp_avg))\n",
    "        #print(len(imp_max))\n",
    "        #print(len(imp_min))\n",
    "                \n",
    "        \n",
    "        # Don't waste a plot if no imputation happened\n",
    "        if not (o[region].equals(imp_avg)):\n",
    "            comparison_demand_plot(region, o, imp_avg, imp_max, imp_min, imp_name, t, s, o_max)\n",
    "        if end == tot_l:\n",
    "            break\n",
    "        k += 1\n",
    "        start += width\n",
    "        end += width\n",
    "        if end >= tot_l:\n",
    "            end = tot_l\n",
    "\n",
    "\n",
    "\n",
    "def comparison_scatter_plot(v1s, v2s, labels, t1, t2, title, save, float_y_min=False):\n",
    "\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_xlabel(t1)\n",
    "    ax.set_ylabel(t2)\n",
    "    max_v1s = np.max( list(map(lambda x: np.max(x), v1s)))\n",
    "    max_v2s = np.max( list(map(lambda x: np.max(x), v2s)))\n",
    "    ax.set_xlim(0, max_v1s*1.1)\n",
    "    min_v2s = 0\n",
    "    if float_y_min:\n",
    "        min_v2s = np.min( list(map(lambda x: np.min(x), v2s)))\n",
    "    ax.set_ylim(min_v2s, max_v2s*1.1)\n",
    "    plt.title(title)\n",
    "    for v1, v2, l in zip(v1s, v2s, labels):\n",
    "        ax.plot(v1, v2, '.', label=l, alpha=0.2)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "def simple_resolution(df_true, df_imp, title, save, n_bins=20):\n",
    "\n",
    "    plt.close()\n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting simple_resolution\")\n",
    "    #    return 0\n",
    "\n",
    "    #res_grid = []\n",
    "    #for index, value in df_true.items():\n",
    "    #    if value > 0:\n",
    "    #        res_grid.append( (df_imp.at[index]-value) / value)\n",
    "    #    else:\n",
    "    #        print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    res_grid = []\n",
    "    for obs, val in zip(df_true, df_imp):\n",
    "        if obs != 0:\n",
    "            res_grid.append((val-obs)/obs)\n",
    "        else:\n",
    "            print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    n, bins, patches = ax.hist(res_grid, n_bins, \n",
    "            facecolor='b', alpha=0.5, density=False)\n",
    "    print(\"Length simp_res {}\".format(np.sum(n)))\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "# df_master is used to get the correct UCT time\n",
    "def resolution_by_time(df_master, idx_vals, df_true, df_imp, title, save):\n",
    "\n",
    "    plt.close()\n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting resolution_by_time\")\n",
    "    #    return 0\n",
    "    \n",
    "    # Get UCT time by index\n",
    "    zero_index_hour = df_master.at[0, 'date_time'].hour\n",
    "    \n",
    "    res_grid = []\n",
    "    for i in range(24):\n",
    "        res_grid.append([])\n",
    "    month_grid = []\n",
    "    for i in range(12):\n",
    "        month_grid.append([])\n",
    "    week_grid = []\n",
    "    for i in range(7):\n",
    "        week_grid.append([])\n",
    "    \n",
    "    for idx, obs, val in zip(idx_vals, df_true, df_imp):\n",
    "        mod = (idx + zero_index_hour - 8)%24 # -8 for PST vs. UCT\n",
    "        res = (val-obs)/obs\n",
    "        res_grid[mod].append(res)\n",
    "        # Get month\n",
    "        month = df_master.at[idx, 'date_time'].month\n",
    "        month_grid[month-1].append(res)\n",
    "        # Get week\n",
    "        day = df_master.at[idx, 'date_time'].weekday()\n",
    "        week_grid[day].append(res)\n",
    "\n",
    "    # Plot hourly\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    plt.grid()\n",
    "    ax.set_title(title+': whiskers at 5%/95%')\n",
    "    ax.boxplot(res_grid, whis=[5, 95])\n",
    "    ax.set_xlabel('Hour (PST)')\n",
    "    ax.set_ylabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    # Plot monthly\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    plt.grid()\n",
    "    ax.set_title(title.replace('hour', 'month')+': whiskers at 5%/95%')\n",
    "    ax.boxplot(month_grid, whis=[5, 95])\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save.replace('hour', 'month'))\n",
    "    \n",
    "    # Plot weekly\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    plt.grid()\n",
    "    ax.set_title(title.replace('hour', 'week')+': whiskers at 5%/95%')\n",
    "    ax.boxplot(week_grid, whis=[5, 95])\n",
    "    ax.set_xlabel('Week Days (1 == Monday, 7 == Sunday)')\n",
    "    ax.set_ylabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save.replace('hour', 'week'))\n",
    "\n",
    "    \n",
    "    \n",
    "def return_all_regions():\n",
    "    return ['AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "    'DUK', 'FMPP', 'FPC',\n",
    "    'FPL', 'GVL', 'HST', 'ISNE',\n",
    "    'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "    'NYIS', 'OVEC', 'PJM', 'SC',\n",
    "    'SCEG', 'SEC', 'SOCO',\n",
    "    'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "    'TVA', 'ERCO',\n",
    "    'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "    'CHPD', 'CISO', 'DOPD',\n",
    "    'EPE', 'GCPD', 'IID',\n",
    "    'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "    'PACE', 'PACW', 'PGE', 'PNM',\n",
    "    'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "    'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "    'WALC', 'WAUW']\n",
    "    \n",
    "    \n",
    "width = 500\n",
    "#width = 10\n",
    "n1 = 'MICE'\n",
    "## Open a saved csv and check contents\n",
    "base1 = '/Users/truggles/Downloads/'\n",
    "base2 = '/Users/truggles/Downloads/'\n",
    "\n",
    "# Chez Ruggles\n",
    "base1 = '/Users/truggles/Downloads/fourty_options_with_plots/'\n",
    "base2 = '/Users/truggles/Downloads/results_from_forty_options/'\n",
    "\n",
    "imp_map = {} # input impute file: returned imputed file,\n",
    "for i in range(40):\n",
    "    imp_map['ca_for_overimpute4_{:d}.csv'.format(i)] = \\\n",
    "            'mean_impute_CA_overimpute4_{:d}_mice.csv'.format(i)\n",
    "\n",
    "### NOTE, if you want ALL regions use regions = return_all_regions()\n",
    "\n",
    "only_scrolling = False\n",
    "if only_scrolling:\n",
    "    print(\"\\nOnly producing the scrolling demand plots\\n\")\n",
    "\n",
    "regs = ['TIDC',]\n",
    "# Loop all regions\n",
    "for r in regions:\n",
    "    if only_scrolling:\n",
    "        break\n",
    "    print(r)\n",
    "    record = {\n",
    "        'idx_all' : [],\n",
    "        'idx_short' : [],\n",
    "        'idx_long' : [],\n",
    "        \n",
    "        'obs_all' : [],\n",
    "        'obs_short' : [],\n",
    "        'obs_long' : [],\n",
    "\n",
    "        'imp_all' : [],\n",
    "        'imp_short' : [],\n",
    "        'imp_long' : [],\n",
    "    }\n",
    "    # Loop all imputed files\n",
    "    all_mice = []\n",
    "    for k, v in imp_map.items():\n",
    "        df_imp = load_csv(base1+k, regions, ['NA',], True, 0)\n",
    "        df_mice = load_csv(base2+v, regions, ['NA',], True, 0)\n",
    "        all_mice.append(df_mice)\n",
    "        \n",
    "        #indices = return_imputed_indices(df, r)\n",
    "\n",
    "        # All imputed points comparing algos against eachother\n",
    "        #v1 = return_values_by_index(df_mice, indices, r)\n",
    "\n",
    "        #comparison_demand_plot(df, [v1,], [n1,],\n",
    "        #    'Imputation of {}'.format(r), 'imp_dem_{}_comp.png'.format(r))\n",
    "        #scrolling_demand(width, r, df, df_mice, 'MICE',\n",
    "        #    'Imputation of {}'.format(r), '/Users/truggles/tmp_plots_imp/imp_dem_{}_comp.png'.format(r))\n",
    "        \n",
    "        \n",
    "        # Find index ONLY from overimputation\n",
    "        over_index = get_overimpute_index(df, df_imp, r)\n",
    "\n",
    "        # Split results by short vs long imputation gaps\n",
    "        short_obs, long_obs = split_index_into_sort_and_long_gaps(over_index)\n",
    "        record['idx_all'].append(over_index)\n",
    "        record['idx_short'].append(short_obs)\n",
    "        record['idx_long'].append(long_obs)\n",
    "        \n",
    "        record['obs_all'].append(return_values_by_index(df, over_index, r))\n",
    "        record['obs_short'].append(return_values_by_index(df, short_obs, r))\n",
    "        record['obs_long'].append(return_values_by_index(df, long_obs, r))\n",
    "        \n",
    "        record['imp_all'].append(return_values_by_index(df_mice, over_index, r))\n",
    "        record['imp_short'].append(return_values_by_index(df_mice, short_obs, r))\n",
    "        record['imp_long'].append(return_values_by_index(df_mice, long_obs, r))\n",
    "\n",
    "    \n",
    "    idxs = ['idx_all', 'idx_short', 'idx_long']\n",
    "    obss = ['obs_all', 'obs_short', 'obs_long']\n",
    "    imps = ['imp_all', 'imp_short', 'imp_long']\n",
    "    names = ['all', 'short', 'long']\n",
    "    for name, idx, obs, imp in zip(names, idxs, obss, imps):\n",
    "        idx_vals = np.concatenate(record[idx])\n",
    "        obs_vals = pd.concat(record[obs])\n",
    "        imp_vals = pd.concat(record[imp])\n",
    "        \n",
    "        simple_resolution(obs_vals, imp_vals, 'overimp resolution {} {} {}'.format(name, n1, r),\n",
    "            'impOver_resolution_{}_{}_{}.png'.format(r, n1, name), 30)\n",
    "        resolution_by_time(df, idx_vals, obs_vals, imp_vals, 'overimp hourly resolution {} {} {}'.format(name, n1, r),\n",
    "            'imp_resolution_hourly_{}_{}_{}.png'.format(r, n1, name))\n",
    "        comparison_scatter_plot([obs_vals.values,],\n",
    "            [imp_vals.values,], [n1+':Obs',], 'Observed', 'Imputed',\n",
    "            'Comparing {} for region: {}'.format(n1, name), 'imp_scatter_overImp_{}_{}_comp.png'.format(r, name))\n",
    "        # Scatter with resolution on y-axis\n",
    "        res1 = []\n",
    "        for val, obs in zip(imp_vals, obs_vals):\n",
    "            res1.append((val-obs)/obs)\n",
    "        float_y_min = True\n",
    "        comparison_scatter_plot([obs_vals.values,],\n",
    "            [res1,], [n1+' Resolution',], 'Observed', '(Imp. - Obs.)/Obs.',\n",
    "            'Resolution {} for region: {}'.format(n1, name), 'imp_scatter_overImp_res_{}_{}_comp.png'.format(r, name), float_y_min)       \n",
    "\n",
    "#for k1, v1 in ordered.items():\n",
    "#    for k2, v2 in v1.items():\n",
    "#        print(k1, k2, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overimpute validation plots with scrolling demand\n",
    "### based on ca_for_overimpute_0.csv\n",
    "### Dave returned 20 imputed copies\n",
    "\n",
    "\n",
    "width = 500\n",
    "#width = 10\n",
    "\n",
    "## Open a saved csv and check contents\n",
    "base1 = '/Users/truggles/Downloads/'\n",
    "base2 = '/Users/truggles/Downloads/overimpute_0/'\n",
    "\n",
    "regs = ['TIDC',]\n",
    "regs = ['CISO',]\n",
    "# Loop all regions\n",
    "for r in regions:\n",
    "    print(r)\n",
    "    df_imp = load_csv(base1+'ca_for_overimpute4_0.csv', regions, ['NA',], True, 0)\n",
    "    # Loop all imputed files\n",
    "    all_mice = []\n",
    "    for i in range(1, 21):\n",
    "        \n",
    "        df_mice = load_csv(base2+'imp_num_{:d}_mice.csv'.format(i), regions, ['NA',], True, 0)\n",
    "        all_mice.append(df_mice)\n",
    "    \n",
    "    scrolling_demand(width, r, df, all_mice, 'MICE',\n",
    "            'Imputation of {}'.format(r), '/Users/truggles/tmp_plots_imp/imp_dem_{}_comp.png'.format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_for_missing_structure(df):\n",
    "    rec = {}\n",
    "    for col in df.columns:\n",
    "        if 'Lag1' in col or 'date_time' in col: continue\n",
    "        rec[col] = [0, []] # missing tally, and record\n",
    "    for index, row in df.iterrows():\n",
    "        for col, info in rec.items():\n",
    "            if np.isnan(row[col]): # incriment missing tally\n",
    "                info[0] += 1\n",
    "            elif not np.isnan(row[col]) and info[0] > 0:\n",
    "                info[1].append(info[0])\n",
    "                info[0] = 0\n",
    "    #for k, v in rec.items():\n",
    "    #    print(k, v)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def remove_locations(df, requested_gaps, cnt=-1):\n",
    "    # Begin with the longest requested gaps and work you way\n",
    "    # to smaller requested gaps\n",
    "    for col, info in requested_gaps.items():\n",
    "        info[1].sort()\n",
    "        info[1].reverse()\n",
    "        #print(col, info[1])\n",
    "        for length in info[1]:\n",
    "            # Try in requested column first the simple way, if that doesn't\n",
    "            # work, try the \"difficult\" way where some existing\n",
    "            # np.nans will be included in naned data\n",
    "            #if find_and_remove_location(df, col, length):\n",
    "            #    find_and_remove_difficult_location(df, col, length, cnt)\n",
    "            find_and_remove_location2(df, col, length)\n",
    "\n",
    "            \n",
    "\n",
    "# Loop over vals in the dataframe and find a continous region which\n",
    "# has a reasonably \"good\" data buffer around the requested length of data to remove.\n",
    "# Change the valse to np.nan\n",
    "def find_and_remove_location2(df, col, length, verbose=False):\n",
    "    # Start at a random index position to not bias the removals\n",
    "    # all towards the front\n",
    "    loc = int(np.random.uniform(0, len(df.index)-length-2))\n",
    "\n",
    "    while True:\n",
    "        # Check that we begin with a \"good\" data value\n",
    "        if not np.isnan(df.at[loc, col]):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Will remove length {} from location with {} NANs\".format(\n",
    "                    length, df.loc[loc:loc+length, col].isna().sum()))\n",
    "            df.loc[loc+1:loc+length, col] = np.nan\n",
    "            if verbose:\n",
    "                print(\"Removed length {} from location with {} NANs\".format(\n",
    "                    length, df.loc[loc:loc+length, col].isna().sum()))\n",
    "            return 0\n",
    "        loc += 1\n",
    "        \n",
    "        # Wrap to start of df before reaching the end to\n",
    "        # ensure that the gap will fit\n",
    "        if loc + length >= len(df.index):\n",
    "            loc = 0\n",
    "\n",
    "\n",
    "# Loop over vals in the dataframe and find a continous region which\n",
    "# has a reasonably \"good\" data buffer around the requested length of data to remove.\n",
    "# Change the valse to np.nan\n",
    "def find_and_remove_location(df, col, length, verbose=False):\n",
    "    # Start at a random index position to not bias the removals\n",
    "    # all towards the front\n",
    "    max_good_data = 0\n",
    "    loc = int(np.random.uniform(0, len(df.index)))\n",
    "    if loc == len(df.index):\n",
    "        loc -= 1\n",
    "    start_of_good_data = loc\n",
    "    length_of_good_data = 0\n",
    "    n_loops = 0\n",
    "    # How much good data on each side of the new gap?\n",
    "    if length <= 100:\n",
    "        tgt_length = 5 * length \n",
    "        buffer = 2 * length # 2x on each side\n",
    "    elif length <= 1000:\n",
    "        tgt_length = 3 * length \n",
    "        buffer = 1 * length # 1x on each side\n",
    "    else:\n",
    "        tgt_length = int(1.5 * length) \n",
    "        buffer = int(0.25 * length) # 0.25x on each side\n",
    "    while True:\n",
    "        if np.isnan(df.at[loc, col]):\n",
    "            start_of_good_data = loc + 1 # This is the following value\n",
    "            # and will continuously incriment if isnan()\n",
    "            length_of_good_data = 0\n",
    "        else: # good data\n",
    "            length_of_good_data += 1\n",
    "            if length_of_good_data > max_good_data:\n",
    "                max_good_data = length_of_good_data\n",
    "\n",
    "        # Remember pandas DataFrame has different slice notation that normal python\n",
    "        # where the terminal value is included in the slice\n",
    "        if length_of_good_data == tgt_length - 1:\n",
    "            if verbose:\n",
    "                print(\"Found a great spot for removal, col {:}, l={:d}, tgt_l={:d}, buffer={:d}, [{:d}:{:d}]\".format(\n",
    "                        col, length, tgt_length, buffer, start_of_good_data, start_of_good_data+length_of_good_data))\n",
    "            strt = start_of_good_data + buffer # Begin nan after good data buffer \n",
    "            end = start_of_good_data + buffer + length - 1\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            df.loc[strt:end, col] = np.nan\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            return 0\n",
    "        loc += 1\n",
    "        \n",
    "        # Wrap to start of df\n",
    "        if loc >= len(df.index):\n",
    "            loc = 0\n",
    "            n_loops += 1\n",
    "            if n_loops > 1:\n",
    "                print(\"Too many loops for col {} and requested length {}, max good data length {}\".format(\n",
    "                    col, length, max_good_data))\n",
    "                return 1\n",
    "\n",
    "            \n",
    "def find_and_remove_difficult_location(df, col, length, to_take=-1, verbose=False):\n",
    "    # Scan data and look for highest purity \"good\" data region\n",
    "    # to apply np.nan\n",
    "    rec = []\n",
    "    print(\"find_and_remove_difficult_location\")\n",
    "    print(\" - Looking for col {} for length {}\".format(col, length))\n",
    "    for i in range(0, int(len(df.index) - 1.5 * length)):\n",
    "        rec.append((i, df.loc[i:int(i+1.5*length), col].isna().sum()))\n",
    "    nan_min = 9999\n",
    "    best_idx = -1\n",
    "    for val in rec:\n",
    "        if val[1] < nan_min:\n",
    "            nan_min = val[1]\n",
    "            best_idx = val[0]\n",
    "\n",
    "    # Find other comparable locations for adding a gap.\n",
    "    # Search for reginons with 20% more less NANs\n",
    "    # with respect to length requested.\n",
    "    others = []\n",
    "    for val in rec:\n",
    "        if val[1] <= length * 0.2 and abs(val[0]-best_idx) > length:\n",
    "            new_gap = True\n",
    "            for other in others:\n",
    "                if abs(val[0]-other[0]) < length:\n",
    "                    new_gap = False\n",
    "            if new_gap and val[0] - length < len(df.index):\n",
    "                others.append(val)\n",
    "    print(\" - others:\")\n",
    "    print(others)\n",
    "    \n",
    "    \n",
    "    print(\" - Best idx {} for nan count of {}\".format(best_idx, nan_min))\n",
    "    if to_take >= 0:\n",
    "        print(others[to_take][0])\n",
    "        print(\" - Will select location based on 'Others' {} {}\".format(to_take, others[to_take]))\n",
    "        df.loc[int(others[to_take][0]+0.25*length):int(others[to_take][0]+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in 'other' location {} with {} total np.nans\".format(\n",
    "            others[to_take][0],\n",
    "            df.loc[others[to_take][0]:int(others[to_take][0]+1.5*length), col].isna().sum()))\n",
    "    else:\n",
    "        df.loc[int(best_idx+0.25*length):int(best_idx+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in defaul location {} with {} total np.nans\".format(\n",
    "            best_idx,\n",
    "            df.loc[best_idx:int(best_idx+1.5*length), col].isna().sum()))\n",
    "            \n",
    "\n",
    "file_path = 'pre_imputed_CA_data.csv'\n",
    "regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "df = load_csv(file_path, regions, ['NA',], True, 2)\n",
    "\n",
    "np.random.seed(1)\n",
    "results = scan_for_missing_structure(df)\n",
    "plt.imshow(~df.isna(), aspect='auto')\n",
    "plt.xlabel(\"variables\")\n",
    "plt.ylabel(\"cases\")\n",
    "plt.gray()\n",
    "plt.savefig('val_ca_for_overimpute3_input.png')\n",
    "\n",
    "make_overimpute = False\n",
    "if make_overimpute:\n",
    "    for i in range(0, 40):\n",
    "        df2 = copy.deepcopy(df)\n",
    "        results2 = copy.deepcopy(results)\n",
    "        remove_locations(df2, results2, i)\n",
    "        plt.imshow(~df2.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.savefig('val_ca_for_overimpute4_{}.png'.format(str(i)))\n",
    "        print(\"Saving as 'ca_for_overimpute4_{}.csv'\".format(str(i)))\n",
    "        df2.to_csv('ca_for_overimpute4_{}.csv'.format(str(i)), index=False, na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
