{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as colors\n",
    "import pickle\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hMinus = 24*2 # Don't use these ones except for testing!\n",
    "#hPlus = 24*2-1 # Don't use these ones except for testing!\n",
    "hMinus = 24\n",
    "hPlus = 24-1\n",
    "hMinusIQR = 24*5\n",
    "hPlusIQR = 24*5-1\n",
    "nDays = 10\n",
    "# based on Pandas slicing\n",
    "\n",
    "def add_rolling_dem(df):\n",
    "    # Can't use np.roll b/c it does not deal with NANs\n",
    "    # in a sophisticated manner.  Use np.nanmean which\n",
    "    # skips all NANs and leaves them out of the sum and\n",
    "    # division\n",
    "    rolling = np.empty((0,), float)\n",
    "    \n",
    "    for i in range(len(df.index)):\n",
    "        val = np.nanmedian(df.loc[i-hMinus:i+hPlus, 'demand (MW)'])\n",
    "        rolling = np.append(rolling, val)\n",
    "    \n",
    "    return df.assign(rollingDem=rolling)\n",
    "\n",
    "\n",
    "def add_rolling_dem_long(df):\n",
    "    # Can't use np.roll b/c it does not deal with NANs\n",
    "    # in a sophisticated manner.  Use np.nanmean which\n",
    "    # skips all NANs and leaves them out of the sum and\n",
    "    # division\n",
    "    rolling = np.empty((0,), float)\n",
    "    \n",
    "    for i in range(len(df.index)):\n",
    "        val = np.nanmedian(df.loc[max(0, i-nDays*24):min(i+nDays*24-1, len(df.index)), 'demand (MW)'])\n",
    "        rolling = np.append(rolling, val)\n",
    "    \n",
    "    return df.assign(rollingDemLong=rolling)\n",
    "\n",
    "def add_demand_minus_rolling_dem(df):\n",
    "    diff = df['demand (MW)'] - df['rollingDem']\n",
    "    df = df.assign(dem_minus_rolling=diff)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_demand_rel_diff_wrt_hourly(df):\n",
    "    diff = df['demand (MW)'] / (df['rollingDem'] * df['hourly_median_dem_dev'])\n",
    "    df = df.assign(dem_rel_diff_wrt_hourly=diff)\n",
    "    diff2 = df['demand (MW)'] / (df['rollingDemLong'] * df['hourly_median_dem_dev'])\n",
    "    df = df.assign(dem_rel_diff_wrt_hourly_long=diff2)\n",
    "    diff3 = df['dem_rel_diff_wrt_hourly'].diff()\n",
    "    df = df.assign(dem_rel_diff_wrt_hourly_delta_pre=diff3)\n",
    "    diff4 = df['dem_rel_diff_wrt_hourly'].diff(periods=-1)\n",
    "    df = df.assign(dem_rel_diff_wrt_hourly_delta_post=diff4)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_demand_minus_rolling_dem_iqr(df):\n",
    "    rolling = np.empty((0,), float)\n",
    "    \n",
    "    for i in range(len(df.index)):\n",
    "        lst = df.loc[i-hMinusIQR:i+hPlusIQR, 'dem_minus_rolling']\n",
    "        iqr = np.nanpercentile(lst, 75) - np.nanpercentile(lst, 25)\n",
    "        rolling = np.append(rolling, iqr)\n",
    "    \n",
    "    return df.assign(dem_minus_rolling_IQR=rolling)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_hourly_median_dem_deviations(df):\n",
    "    # Create a df to hold all values to take nanmedian later\n",
    "    vals_dem_minus_rolling = df['dem_minus_rolling']\n",
    "    # Loop over nDays days on each side\n",
    "    for i in range(-nDays, nDays+1):\n",
    "        # Already initialized with zero value\n",
    "        if i == 0:\n",
    "            continue\n",
    "        vals_dem_minus_rolling = pd.concat(\n",
    "            [vals_dem_minus_rolling, df.shift(periods=i*24)['dem_minus_rolling']], axis=1)\n",
    "\n",
    "    df['vals_dem_minus_rolling'] = vals_dem_minus_rolling.median(axis=1, skipna=True)\n",
    "    # 1+vals to make it a scale factor\n",
    "    return df.assign(hourly_median_dem_dev=1.+df['vals_dem_minus_rolling']/df['rollingDemLong'])\n",
    "\n",
    "                \n",
    "                \n",
    "# delta with previous and following time steps\n",
    "def add_deltas(df):\n",
    "    diff = df['demand (MW)'].diff()\n",
    "    df = df.assign(delta_pre=diff)\n",
    "    diff = df['demand (MW)'].diff(periods=-1)\n",
    "    df = df.assign(delta_post=diff)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_rolling_delta_iqr(df):\n",
    "    rolling = np.empty((0,), float)\n",
    "    \n",
    "    for i in range(len(df.index)):\n",
    "        lst = df.loc[i-hMinusIQR:i+hPlusIQR, 'delta_pre']\n",
    "        iqr = np.nanpercentile(lst, 75) - np.nanpercentile(lst, 25)\n",
    "        rolling = np.append(rolling, iqr)\n",
    "    \n",
    "    return df.assign(delta_rolling_IQR=rolling)\n",
    "\n",
    "\n",
    "def add_categories(df):\n",
    "    df['category'] = np.where(df['demand (MW)'].isna(), 'MISSING', 'OKAY')\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_neg_and_zeros(df):\n",
    "    df['category'] = np.where(df['demand (MW)'] <= 0., 'NEG_OR_ZERO', df['category'])\n",
    "    filtered = np.where(df['demand (MW)'] <= 0., df['demand (MW)'], np.nan)\n",
    "    df['negAndZeroFiltered'] = filtered\n",
    "    df['demand (MW)'] = df['demand (MW)'].mask(df['demand (MW)'] <= 0.)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def filter_extrem_demand(df, multiplier):\n",
    "    med = np.nanmedian(df['demand (MW)'])\n",
    "    filtered = df['demand (MW)'].where(df['demand (MW)'] < med * multiplier)\n",
    "    df['globalDemandFiltered'] = np.where(df['demand (MW)'] != filtered, df['demand (MW)'], np.nan)\n",
    "    df['category'] = df['category'].mask(((df['demand (MW)'] != filtered) & \\\n",
    "                    (df['demand (MW)'].notna())), other='GLOBAL_DEM')\n",
    "    df['demand (MW)'] = filtered\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_global_plus_minus_one(df):\n",
    "    globalDemPlusMinusFiltered = [np.nan for _ in df.index]\n",
    "    for idx in df.index:\n",
    "        if df.loc[idx, 'category'] == 'GLOBAL_DEM':\n",
    "            if df.loc[idx-1, 'category'] == 'OKAY':\n",
    "                df.loc[idx-1, 'category'] = 'GLOBAL_DEM_PLUS_MINUS'\n",
    "                globalDemPlusMinusFiltered[idx-1] = df.loc[idx-1, 'demand (MW)']\n",
    "                df.loc[idx-1, 'demand (MW)'] = np.nan\n",
    "            if df.loc[idx+1, 'category'] == 'OKAY':\n",
    "                df.loc[idx+1, 'category'] = 'GLOBAL_DEM_PLUS_MINUS'\n",
    "                globalDemPlusMinusFiltered[idx+1] = df.loc[idx+1, 'demand (MW)']\n",
    "                df.loc[idx+1, 'demand (MW)'] = np.nan\n",
    "    df['globalDemPlusMinusFiltered'] = globalDemPlusMinusFiltered\n",
    "    return df\n",
    "    \n",
    "\n",
    "def filter_local_demand(df, multiplier_up, multiplier_down):\n",
    "    \n",
    "    # Filter in two steps to provide different labels for the categories\n",
    "    filtered = df['demand (MW)'].where(\n",
    "            (df['demand (MW)'] < df['rollingDem'] * df['hourly_median_dem_dev'] + \\\n",
    "                     multiplier_up * df['dem_minus_rolling_IQR']))\n",
    "    df['localDemandFilteredUp'] = np.where(df['demand (MW)'] != filtered, df['demand (MW)'], np.nan)\n",
    "    df['category'] = df['category'].mask(((df['demand (MW)'] != filtered) & \\\n",
    "                    (df['demand (MW)'].notna())), other='LOCAL_DEM_UP')\n",
    "    df['demand (MW)'] = filtered\n",
    "    \n",
    "    filtered = df['demand (MW)'].where(\n",
    "            (df['demand (MW)'] > df['rollingDem'] * df['hourly_median_dem_dev'] - \\\n",
    "                     multiplier_down * df['dem_minus_rolling_IQR']))\n",
    "    df['localDemandFilteredDown'] = np.where(df['demand (MW)'] != filtered, df['demand (MW)'], np.nan)\n",
    "    df['category'] = df['category'].mask(((df['demand (MW)'] != filtered) & \\\n",
    "                    (df['demand (MW)'].notna())), other='LOCAL_DEM_DOWN')\n",
    "    df['demand (MW)'] = filtered\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Filter on a multiplier of the IQR and set\n",
    "# the associated 'demand (MW)' value to NAN.\n",
    "# Only consider \"double deltas\", hours with\n",
    "# large deltas on both sides\n",
    "def filter_deltas(df, multiplier):\n",
    "    \n",
    "    filtered = df['demand (MW)'].mask(\n",
    "            ((df['delta_pre'] > df['delta_rolling_IQR'] * multiplier) & \\\n",
    "            (df['delta_post'] > df['delta_rolling_IQR'] * multiplier)) | \\\n",
    "            ((df['delta_pre'] < -1. * df['delta_rolling_IQR'] * multiplier) & \\\n",
    "            (df['delta_post'] < -1. * df['delta_rolling_IQR'] * multiplier)))\n",
    "\n",
    "    df['deltaFiltered'] = np.where(df['demand (MW)'] != filtered, df['demand (MW)'], np.nan)\n",
    "    df['category'] = df['category'].mask(((df['demand (MW)'] != filtered) & \\\n",
    "                    (df['demand (MW)'].notna())), other='DELTA')\n",
    "    df['demand (MW)'] = filtered\n",
    "    return df\n",
    "\n",
    "\n",
    "# March through all hours recording previous \"good\"\n",
    "# demand value and its index.  Calculate deltas between\n",
    "# this value and next \"good\" hour.  If delta is LARGE\n",
    "# mark NAN.\n",
    "# Go forwards once, then backwards once to get all options.\n",
    "def filter_deltas_marching(df, multiplier, rel_multiplier):\n",
    "    \n",
    "    # This is a global value, but it is of a relative quantity\n",
    "    # so should scale as demand expectations change.\n",
    "    iqr_delta_pre = np.nanpercentile(df['dem_rel_diff_wrt_hourly_delta_pre'], 75) - \\\n",
    "            np.nanpercentile(df['dem_rel_diff_wrt_hourly_delta_pre'], 25)\n",
    "\n",
    "    # Go through forwards first, then reverse\n",
    "    prev_good_index = np.nan\n",
    "    \n",
    "    deltaSingleFiltered = []\n",
    "    for idx in df.index:\n",
    "        deltaSingleFiltered.append(np.nan)\n",
    "        if np.isnan(df.loc[idx, 'demand (MW)']):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Initialize first good entry, this will never be flagged\n",
    "        if np.isnan(prev_good_index):\n",
    "            prev_good_index = idx\n",
    "            \n",
    "        \n",
    "        # Check deltas demand and relative wrt hourly adjustment\n",
    "        prev_good_delta_dem = abs(df.loc[prev_good_index, 'demand (MW)'] - df.loc[idx, 'demand (MW)'])\n",
    "        prev_good_delta_dem_rel_diff_wrt_hourly = abs(df.loc[prev_good_index, \n",
    "                        'dem_rel_diff_wrt_hourly'] - df.loc[idx, 'dem_rel_diff_wrt_hourly'])\n",
    "\n",
    "        \n",
    "        # delta_rolling_IQR is over 5 days on each side so should be\n",
    "        # similar regardless of which hours' we use. If delta is\n",
    "        # large, mark this hour anomalous\n",
    "        if (prev_good_delta_dem > df.loc[idx, 'delta_rolling_IQR'] * multiplier) and \\\n",
    "                (prev_good_delta_dem_rel_diff_wrt_hourly > rel_multiplier * iqr_delta_pre):\n",
    "            \n",
    "            \n",
    "            # If the previous \"good\" value was farther from expected values, then consider current hour good\n",
    "            # and the previous hour will be caught on the way back through the reverse direction.\n",
    "            # The max deviation from the rolling 4 day dem and the rolling 10 day dem is taken\n",
    "            # to help catch cases where a large deviation pulls the rolling 4 day dem to center\n",
    "            # on its values.  i.e. SCL 2016 Dec 15.\n",
    "            prev_max = max(abs(1. - df.loc[prev_good_index, 'dem_rel_diff_wrt_hourly']),\n",
    "                            abs(1. - df.loc[prev_good_index, 'dem_rel_diff_wrt_hourly_long']))\n",
    "            current_max = max(abs(1. - df.loc[idx, 'dem_rel_diff_wrt_hourly']),\n",
    "                            abs(1. - df.loc[idx, 'dem_rel_diff_wrt_hourly_long']))         \n",
    "            if abs(current_max) < abs(prev_max):\n",
    "                prev_good_index = idx\n",
    "            \n",
    "            # else, continue to filter this hour\n",
    "            else:\n",
    "                deltaSingleFiltered[-1] = df.loc[idx, 'demand (MW)']\n",
    "                df.loc[idx, 'demand (MW)'] = np.nan\n",
    "                df.loc[idx, 'category'] = 'SINGLE_DELTA'\n",
    "        else:\n",
    "            prev_good_index = idx\n",
    "\n",
    "    \n",
    "    df['deltaSingleFilteredFwd'] = deltaSingleFiltered\n",
    "    \n",
    "    \n",
    "    ### Go through reversed, ~ copy of above code ###\n",
    "    prev_good_index = np.nan\n",
    "    \n",
    "    deltaSingleFiltered = []\n",
    "    for idx in reversed(df.index):\n",
    "        deltaSingleFiltered.append(np.nan)\n",
    "        if np.isnan(df.loc[idx, 'demand (MW)']):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # Initialize first good entry, this will never be flagged\n",
    "        if np.isnan(prev_good_index):\n",
    "            prev_good_index = idx\n",
    "            \n",
    "        \n",
    "        # Check deltas demand and relative wrt hourly adjustment\n",
    "        prev_good_delta_dem = abs(df.loc[prev_good_index, 'demand (MW)'] - df.loc[idx, 'demand (MW)'])\n",
    "        prev_good_delta_dem_rel_diff_wrt_hourly = abs(df.loc[prev_good_index, \n",
    "                        'dem_rel_diff_wrt_hourly'] - df.loc[idx, 'dem_rel_diff_wrt_hourly'])\n",
    "\n",
    "        \n",
    "        \n",
    "        # delta_rolling_IQR is over 5 days on each side so should be\n",
    "        # similar regardless of which hours' we use. If delta is\n",
    "        # large, mark this hour anomalous\n",
    "        if (prev_good_delta_dem > df.loc[idx, 'delta_rolling_IQR'] * multiplier) and \\\n",
    "                (prev_good_delta_dem_rel_diff_wrt_hourly > rel_multiplier * iqr_delta_pre):\n",
    "            \n",
    "            \n",
    "            deltaSingleFiltered[-1] = df.loc[idx, 'demand (MW)']\n",
    "            df.loc[idx, 'demand (MW)'] = np.nan\n",
    "            df.loc[idx, 'category'] = 'SINGLE_DELTA'\n",
    "        else:\n",
    "            prev_good_index = idx\n",
    "\n",
    "    \n",
    "    to_app = [val for val in reversed(deltaSingleFiltered)]\n",
    "    df['deltaSingleFilteredBkw'] = to_app\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "  \n",
    "            \n",
    "\n",
    "def filter_runs(df):\n",
    "    \n",
    "    d1 = df['demand (MW)'].diff(periods=1)\n",
    "    d2 = df['demand (MW)'].diff(periods=2)\n",
    "\n",
    "    # cannot compare a dtyped [float64] array with a scalar of type [bool]\n",
    "    filtered = df['demand (MW)'].mask((d1 == 0) & (d2 == 0))\n",
    "    df['runFiltered'] = np.where(df['demand (MW)'] != filtered, df['demand (MW)'], np.nan)\n",
    "    df['demand (MW)'] = filtered\n",
    "    df['category'] = np.where(df['runFiltered'].notna(), 'IDENTICAL_RUN', df['category'])\n",
    "    return df\n",
    "    \n",
    "\n",
    "    \n",
    "def filter_anomalous_regions(df, width, anomalous_pct):\n",
    "    \n",
    "    print(\"filter_anomalous_regions\")\n",
    "    \n",
    "    percent_good_data_cnt = [0. for _ in df.index]\n",
    "    percent_good_data_pre = [0. for _ in df.index]\n",
    "    percent_good_data_post = [0. for _ in df.index]\n",
    "    df['len_good_data'] = [0 for _ in df.index]\n",
    "    data_quality_cnt = []\n",
    "    data_quality_short = []\n",
    "    start_good_data = np.nan\n",
    "    end_good_data = np.nan\n",
    "    for idx in df.index:\n",
    "            \n",
    "        # Remove the oldest item in the list\n",
    "        if len(data_quality_short) > width:\n",
    "            data_quality_short.pop(0)\n",
    "        if len(data_quality_cnt) > 2 * width:\n",
    "            data_quality_cnt.pop(0)\n",
    "        \n",
    "        # Add new item and don't count MISSING as 'bad' data\n",
    "        if df.loc[idx, 'category'] == 'OKAY' or df.loc[idx, 'category'] == 'MISSING':\n",
    "            data_quality_cnt.append(1)\n",
    "            data_quality_short.append(1)\n",
    "            # Track length of good data chunks\n",
    "            if np.isnan(start_good_data):\n",
    "                start_good_data = idx\n",
    "            end_good_data = idx\n",
    "        else:\n",
    "            data_quality_cnt.append(0)\n",
    "            data_quality_short.append(0)\n",
    "            # Fill in length of good data chunk\n",
    "            if not (np.isnan(start_good_data) or np.isnan(end_good_data)):\n",
    "                len_good = end_good_data - start_good_data + 1\n",
    "                df.loc[start_good_data:end_good_data, 'len_good_data'] = len_good\n",
    "            start_good_data = np.nan\n",
    "            end_good_data = np.nan\n",
    "\n",
    "        \n",
    "        # centered measurements have length 2 * width\n",
    "        if len(data_quality_cnt) > 2 * width:\n",
    "            percent_good_data_cnt[idx-width] = np.mean(data_quality_cnt)\n",
    "        # left and right / pre and post measurements have length = width + 1\n",
    "        if len(data_quality_short) > width:\n",
    "            percent_good_data_pre[idx] = np.mean(data_quality_short)\n",
    "            percent_good_data_post[idx-width] = np.mean(data_quality_short)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    anomalousRegionsFiltered = [np.nan for _ in df.index]\n",
    "    for idx in df.index:\n",
    "        if percent_good_data_cnt[idx] <= anomalous_pct:\n",
    "            for j in range(idx-width, idx+width):\n",
    "                if j < 1 or j >= len(df.index):\n",
    "                    continue\n",
    "                if df.loc[j, 'category'] == 'OKAY':\n",
    "                    # If this is the start or end of continuous good data, don't filter\n",
    "                    if percent_good_data_pre[j] == 1.0 or percent_good_data_post[j] == 1.0:\n",
    "                        continue\n",
    "                    if df.loc[j, 'len_good_data'] > width:\n",
    "                        continue\n",
    "                    df.loc[j, 'category'] = 'ANOMALOUS_REGION'\n",
    "                    anomalousRegionsFiltered[j] = df.loc[j, 'demand (MW)']\n",
    "                    df.loc[j, 'demand (MW)'] = np.nan\n",
    "    \n",
    "\n",
    "    df['anomalousRegionsFiltered'] = anomalousRegionsFiltered\n",
    "    return df\n",
    "    \n",
    "\n",
    "def mark_missing_and_empty(df, col):\n",
    "    #marked = np.zeros(len(df.index))\n",
    "    print(df[col].isna())\n",
    "\n",
    "def show_structure(df):\n",
    "    plt.imshow(~df.isna(), aspect='auto')\n",
    "    plt.xlabel(\"variables\")\n",
    "    plt.ylabel(\"cases\")\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def simple_hist(col, df, iq2, iq3, factor, save, x_log=False):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if df[col].max() == np.Inf:\n",
    "        print(save, df[col].max())\n",
    "        return\n",
    "    if df[col].min() == np.NINF:\n",
    "        print(save, df[col].min())\n",
    "        return\n",
    "    n, bins, patches = ax.hist(df[col] * (~df['demand (MW)'].isna()), 100, facecolor='red', alpha=0.2, label='pre')\n",
    "    #n, bins, patches = ax.hist(df['delta_post'], 100, facecolor='blue', alpha=0.2, label='post')\n",
    "    if col == 'Demand (MW)':\n",
    "        ax.set_xlabel('Demand (MW)')\n",
    "    elif col == 'dem_diff_norm_rolling':\n",
    "        ax.set_xlabel('$\\Delta$(Demand, Rolling Avg)/Rolling IQR')\n",
    "    elif col == 'dem_minus_rolling':\n",
    "        ax.set_xlabel('$\\Delta$(Demand, Rolling Avg) (MW)')\n",
    "    elif col == 'delta_pre':\n",
    "        ax.set_xlabel('$\\Delta$(Demand ti, Demand ti-1) (MW)')\n",
    "    elif col == 'delta_pre_norm':\n",
    "        ax.set_xlabel('Normalized Demand Difference (diff/Rolling IQR)')\n",
    "    elif col == 'diff_norm_diffIQR_D':\n",
    "        ax.set_xlabel('Normalized Demand Difference ($\\Delta$(t-1, t+1)/Rolling IQR)')\n",
    "    ax.set_ylabel('Counts')\n",
    "            \n",
    "    # Draw iq2 and iq3\n",
    "    iqr = iq3 - iq2\n",
    "    iq2_l1 = mlines.Line2D([-iqr,-iqr], ax.get_ylim())\n",
    "    ax.add_line(iq2_l1)\n",
    "    iq2_l2 = mlines.Line2D([-iqr*factor,-iqr*factor], ax.get_ylim())\n",
    "    ax.add_line(iq2_l2)\n",
    "    iq3_l1 = mlines.Line2D([iqr,iqr], ax.get_ylim())\n",
    "    ax.add_line(iq3_l1)\n",
    "    iq3_l2 = mlines.Line2D([iqr*factor,iqr*factor], ax.get_ylim())\n",
    "    ax.add_line(iq3_l2)\n",
    "    \n",
    "    if x_log:\n",
    "        plt.xscale('log', nonposx='clip')\n",
    "    plt.tight_layout()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Create many demand plots so we can actually see the values\n",
    "def scrolling_demand(width, region, df, title, save, dem_up, dem_down, delta_up, delta_down, targets=[-1,], plus=0):\n",
    "    #plus = 0\n",
    "    #plus = 225\n",
    "    #plus = 100\n",
    "    start = 0 + plus\n",
    "    end = width+plus\n",
    "    i = 0\n",
    "    tot_l = len(df.index)\n",
    "    while True:\n",
    "        s = save.replace('.png', '_{}cnt_{}plus'.format(i, plus))\n",
    "        # Make names ordered by timeslice if we have target slices\n",
    "        if targets[0] != -1:\n",
    "            s = s.replace(region, '{}_{}'.format(i, region))\n",
    "        t = title+': cnt {}'.format(i)\n",
    "        o = df.loc[start:end]\n",
    "\n",
    "        #print(\"scrolling: start {} - end {}\".format(start, end))\n",
    "        # If requested, only plot certain time frames\n",
    "        if i not in targets and targets[0] != -1:\n",
    "            if end == tot_l:\n",
    "                break\n",
    "            i += 1\n",
    "            start += width\n",
    "            end += width\n",
    "            if end >= tot_l:\n",
    "                end = tot_l\n",
    "            continue\n",
    "        #comparison_demand_plot(o, t, s, dem_up, dem_down)\n",
    "        #comparison_diff_plot(o, t, s.replace('cnt', 'cnt_diff'), delta_up, delta_down)\n",
    "        # end-start+1 is the length, remember pandas slice notation includes end point\n",
    "        if not (\n",
    "                (df['globalDemandFiltered'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['localDemandFilteredUp'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['localDemandFilteredDown'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['runFiltered'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['deltaFiltered'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "            \n",
    "                (df['anomalousRegionsFiltered'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "            \n",
    "                (df['deltaSingleFilteredFwd'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['deltaSingleFilteredBkw'].loc[start:end].isna().sum() == len(o.index)) and \\\n",
    "                (df['negAndZeroFiltered'].loc[start:end].isna().sum() == len(o.index))\n",
    "               ):\n",
    "            s = save.replace('.png', '_{}cnt_{}plus'.format(i, plus)).replace(region, 'z3_'+region)\n",
    "            # Make names ordered by timeslice if we have target slices\n",
    "            if targets[0] != -1:\n",
    "                s = s.replace(region, '{}_{}'.format(i, region))\n",
    "            print(start, end, s)\n",
    "            #comparison_demand_plot(o, t, s, dem_up, dem_down)\n",
    "            comparison_demand_plot_clean(o, t, s, dem_up, dem_down)\n",
    "            #comparison_diff_plot(o, t, s.replace('cnt', 'cnt_diff'), delta_up, delta_down)\n",
    "        if end == tot_l:\n",
    "            break\n",
    "        i += 1\n",
    "        start += width\n",
    "        end += width\n",
    "        if end >= tot_l:\n",
    "            end = tot_l\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "def comparison_demand_plot(df, title, save, multiplier_up, multiplier_down):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('Demand')\n",
    "    plt.title(title)\n",
    "    ax.plot(df['date_time'], df['demand (MW)'], 'kv-', label='demand', linewidth=3.0)\n",
    "    ax.plot(df['date_time'], df['rollingDemLong'], 'r-.', label='rolling dem long')\n",
    "    ax.plot(df['date_time'], df['rollingDem'], 'b-', label='rolling dem+/-')\n",
    "    ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev'], 'b--')\n",
    "    ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev']+multiplier_up*df['dem_minus_rolling_IQR'], 'b-.')\n",
    "    ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev']-multiplier_down*df['dem_minus_rolling_IQR'], 'b-.')\n",
    "    ax.plot(df['date_time'], df['localDemandFilteredUp'], 'mo', label='localDemandFiltered')\n",
    "    ax.plot(df['date_time'], df['localDemandFilteredDown'], 'mo', label='_nolegend_')\n",
    "    ax.plot(df['date_time'], df['globalDemandFiltered']*0., 'co', label='globalDemandFiltered')\n",
    "    ax.plot(df['date_time'], df['globalDemPlusMinusFiltered'], 'co', label='_nolegend_')\n",
    "    ax.plot(df['date_time'], df['deltaFiltered'], 'go', label='deltaFiltered')\n",
    "    ax.plot(df['date_time'], df['deltaSingleFilteredFwd'], 'ro', label='deltaSingleFiltered')\n",
    "    ax.plot(df['date_time'], df['deltaSingleFilteredBkw'], 'rv', label='_nolegend_')\n",
    "    ax.plot(df['date_time'], df['runFiltered'], 'yo', label='runFiltered')\n",
    "    ax.plot(df['date_time'], df['anomalousRegionsFiltered'], 'cv', label='anomalousRegionsFiltered')\n",
    "    plt.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    ax.set_ylim(min(ax.get_ylim()[0], 0), ax.get_ylim()[1])\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "# Shows simplified, clean results     \n",
    "def comparison_demand_plot_clean(df, title, save, multiplier_up, multiplier_down):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,7))\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    #ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('Demand (MW)')\n",
    "    ax.yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    plt.title(title)\n",
    "    if 'PACW' in save:\n",
    "        plt.title('PacifiCorp West') # PACW\n",
    "    if 'SWPP' in save:\n",
    "        plt.title('Southwest Power Pool') # SWPP\n",
    "    if 'SOCO' in save:\n",
    "        plt.title('Southern Company Services, Inc. - Trans') # (SOCO)\n",
    "    if 'SCL' in save:\n",
    "        plt.title('Seattle City Light') # (SCL)\n",
    "    if 'NSB' in save:\n",
    "        plt.title('Utilities Commission of New Smyrna Beach') # (NSB)\n",
    "    plt.title('')\n",
    "    \n",
    "    ax.plot(df['date_time'], df['demand (MW)'], 'k-o', label='Demand', linewidth=3.0)\n",
    "    #ax.plot(df['date_time'], df['rollingDemLong'], 'r-.', label='rolling dem long')\n",
    "    #ax.plot(df['date_time'], df['rollingDem'], 'b-', label='rolling dem+/-')\n",
    "    #ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev'], 'b--')\n",
    "    #ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev']+multiplier_up*df['dem_minus_rolling_IQR'], 'b-.')\n",
    "    #ax.plot(df['date_time'], df['rollingDem']*df['hourly_median_dem_dev']-multiplier_down*df['dem_minus_rolling_IQR'], 'b-.')\n",
    "    \n",
    "    s=9\n",
    "    ax.plot(df['date_time'], df['runFiltered'], 'C0o', label='Identical Run', markersize=s)\n",
    "    ax.plot(df['date_time'], df['globalDemandFiltered']*0., 'C5o', label='Global Demand', markersize=s)\n",
    "    ax.plot(df['date_time'], df['globalDemPlusMinusFiltered'], 'C5o', label='_nolegend_', markersize=s)\n",
    "    ax.plot(df['date_time'], df['deltaFiltered'], 'C2o', label='Double-Sided Delta', markersize=s)\n",
    "    ax.plot(df['date_time'], df['deltaSingleFilteredFwd'], 'C3o', label='Single-Sided Delta', markersize=s)\n",
    "    ax.plot(df['date_time'], df['deltaSingleFilteredBkw'], 'C3o', label='_nolegend_', markersize=s)\n",
    "    ax.plot(df['date_time'], df['localDemandFilteredUp'], 'C4o', label='Local Demand', markersize=s)\n",
    "    ax.plot(df['date_time'], df['localDemandFilteredDown'], 'C4o', label='_nolegend_', markersize=s)\n",
    "    ax.plot(df['date_time'], df['negAndZeroFiltered'], 'C6o', label='Negitive or Zero', markersize=s)\n",
    "    ax.plot(df['date_time'], df['anomalousRegionsFiltered'], 'C1o', label='Anomalous Region', markersize=s)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    plt.legend(prop={'size': 20}, ncol=3)\n",
    "    #ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=45 )\n",
    "    #ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    ax.set_ylim(min(ax.get_ylim()[0], 0), ax.get_ylim()[1]*1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def comparison_diff_plot(df, title, save, multiplier_up, multiplier_down):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('$\\Delta$(Demand ti, ti-1) (MW)')\n",
    "    plt.title(title)\n",
    "    ax.plot(df['date_time'], df['delta_pre'], 'k-', label='delta_pre')\n",
    "    ax.plot(df['date_time'], multiplier_up*df['delta_rolling_IQR'], 'b-.')\n",
    "    ax.plot(df['date_time'], -multiplier_down*df['delta_rolling_IQR'], 'b-.')\n",
    "    plt.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "\n",
    "    \n",
    "    \n",
    "def get_iqrs(vals):\n",
    "    iq3 = np.nanpercentile(vals, 75)\n",
    "    iq2 = np.nanpercentile(vals, 25)\n",
    "    iqr = iq3 - iq2\n",
    "    return iqr, iq2, iq3\n",
    "\n",
    "\n",
    "def return_all_regions():\n",
    "    return ['AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "    'DUK', 'FMPP', 'FPC',\n",
    "    'FPL', 'GVL', 'HST', 'ISNE',\n",
    "    'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "    'NYIS', 'OVEC', 'PJM', 'SC',\n",
    "    'SCEG', 'SEC', 'SOCO',\n",
    "    'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "    'TVA', 'ERCO',\n",
    "    'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "    'CHPD', 'CISO', 'DOPD',\n",
    "    'EPE', 'GCPD', 'IID',\n",
    "    'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "    'PACE', 'PACW', 'PGE', 'PNM',\n",
    "    'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "    'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "    'WALC', 'WAUW']\n",
    "\n",
    "def plot_var_by_time(df, region, var, include_dem=True):\n",
    "    print(\"Plotting reg {} var {}\".format(region, var))\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    if include_dem:\n",
    "        ax.plot(df['date_time'], df['demand (MW)'], 'k-', label='Demand')\n",
    "    ax.plot(df['date_time'], df[var], 'r-', label=var)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.title(\"{} {}\".format(region, var))\n",
    "    plt.savefig('plt/{}_{}.png'.format(region, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_dem_cut = 10\n",
    "local_dem_cut_up = 3.5\n",
    "local_dem_cut_down = 2.5\n",
    "delta_multiplier = 2\n",
    "delta_single_multiplier = 5\n",
    "rel_multiplier = 15\n",
    "anomalous_regions_width = 24\n",
    "anomalous_pct = .85\n",
    "\n",
    "\n",
    "dump_to_pickle = False\n",
    "second_filter_and_save = False\n",
    "load_from_pickle = True\n",
    "version = '_v12_2day'\n",
    "\n",
    "app = 'Oct15'\n",
    "\n",
    "png_path = '/Users/truggles/tmp_plots{}_{}_Jan09/'.format(version, app)\n",
    "if not os.path.isdir(png_path): os.mkdir(png_path)\n",
    "data_path = f'/Users/truggles/tmp_data{app}/'\n",
    "if not os.path.isdir(data_path): os.mkdir(data_path)\n",
    "\n",
    "regions = return_all_regions()\n",
    "\n",
    "\n",
    "#regions = ['CENT', 'MIDW', 'TEN', 'SE', 'FLA', 'CAR', 'MIDA', 'NY', 'NE', 'TEX', 'CAL', 'NW', 'SW']\n",
    "#regions = ['CONUS',]\n",
    "\n",
    "#regions = ['CONUS_from_BAs_for_MEM', 'EASTERN_from_BAs_for_MEM', \n",
    "#           'TEXAS_from_BAs_for_MEM', 'WESTERN_from_BAs_for_MEM']\n",
    "\n",
    "#regions = ['SOCO',]\n",
    "#regions = ['NSB','SWPP']\n",
    "#regions = ['SCL','SOCO','NSB','SWPP']\n",
    "regions = ['FPC',]#'JEA']\n",
    "regions = ['FPC','JEA','WALC','SPA','NWMT','IID']\n",
    "\n",
    "regions.sort()\n",
    "for region in regions:\n",
    "    print(region)\n",
    "\n",
    "    dem_map = {}\n",
    "    if dump_to_pickle:\n",
    "        file_path = '~/data4/{}.csv'.format(region)\n",
    "        dem_map[region] = pd.read_csv(file_path,\n",
    "                           dtype={'demand (MW)':np.float64},\n",
    "                          parse_dates=True, na_values=['MISSING', 'EMPTY'])\n",
    "        \n",
    "        # This is only for checking REGIONS and CONUS after aggregating imputed data\n",
    "        #dem_map[region]['demand (MW)'] = dem_map[region]['cleaned demand (MW)']\n",
    "        \n",
    "        # Convert date/time\n",
    "        dem_map[region]['time'] = pd.to_datetime(dem_map[region]['time'])\n",
    "        dem_map[region]['date_time'] = dem_map[region]['time']\n",
    "        \n",
    "        # Drop unused columns\n",
    "        dem_map[region] = dem_map[region].drop(['time','year','month','day',\n",
    "                                                'hour','forecast demand (MW)'], axis=1)\n",
    "\n",
    "        # Add categories to track filtering\n",
    "        dem_map[region] = add_categories(dem_map[region])\n",
    "        \n",
    "        # Missing and empty values are marked\n",
    "        dem_map[region] = dem_map[region].assign(missing=dem_map[region]['demand (MW)'].isna())\n",
    "\n",
    "        # Set all negative and zero values to NAN\n",
    "        dem_map[region] = filter_neg_and_zeros(dem_map[region])\n",
    "        \n",
    "        # Set last demand values in runs of 3+ to NAN\n",
    "        dem_map[region] = filter_runs(dem_map[region])\n",
    "\n",
    "        # Global demand filter on 10x the median value\n",
    "        dem_map[region] = filter_extrem_demand(dem_map[region], global_dem_cut)\n",
    "        \n",
    "        # Filter +/- 1 hour from any global deman filtered hours\n",
    "        dem_map[region] = filter_global_plus_minus_one(dem_map[region])\n",
    "        \n",
    "\n",
    "        # Add rolling dem average\n",
    "        dem_map[region] = add_rolling_dem(dem_map[region])\n",
    "        dem_map[region] = add_rolling_dem_long(dem_map[region])\n",
    "        dem_map[region] = add_demand_minus_rolling_dem(dem_map[region])\n",
    "        dem_map[region] = add_hourly_median_dem_deviations(dem_map[region])\n",
    "        dem_map[region] = add_demand_minus_rolling_dem_iqr(dem_map[region])\n",
    "\n",
    "        \n",
    "\n",
    "        # Add deltas\n",
    "        dem_map[region] = add_deltas(dem_map[region])\n",
    "        dem_map[region] = add_rolling_delta_iqr(dem_map[region])\n",
    "   \n",
    "    \n",
    "\n",
    "        #dem_map[region] = filter_local_demand(dem_map[region], local_dem_cut_up, local_dem_cut_down)\n",
    "        #dem_map[region] = filter_deltas(dem_map[region], delta_multiplier)\n",
    "        \n",
    "        print('Saving pickle /Users/truggles/tmp_data{}/pickle_{}{}.pkl'.format(app, region, version))\n",
    "        pickle_file = open('/Users/truggles/tmp_data{}/pickle_{}{}.pkl'.format(app, region, version), 'wb') \n",
    "        pickle.dump(dem_map[region], pickle_file)\n",
    "        pickle_file.close()\n",
    "        #dem_map[region].to_csv('/Users/truggles/tmp_data2/csv_{}.csv'.format(region))\n",
    "        #continue\n",
    "\n",
    "        \n",
    "    if second_filter_and_save:\n",
    "        print('Loading from pickle /Users/truggles/tmp_data{}/pickle_{}{}.pkl'.format(app, region, version))\n",
    "        pickle_in = open('/Users/truggles/tmp_data{}/pickle_{}{}.pkl'.format(app, region, version),'rb')\n",
    "        dem_map[region] = pickle.load(pickle_in)\n",
    "        dem_map[region]['date_time'] = pd.to_datetime(dem_map[region]['date_time'])\n",
    "        \n",
    "        dem_map[region] = filter_local_demand(dem_map[region], local_dem_cut_up, local_dem_cut_down)\n",
    "        dem_map[region] = filter_deltas(dem_map[region], delta_multiplier)\n",
    "        dem_map[region] = add_demand_rel_diff_wrt_hourly(dem_map[region])\n",
    "        dem_map[region] = filter_deltas_marching(dem_map[region], delta_single_multiplier, rel_multiplier)\n",
    "        \n",
    "        dem_map[region] = filter_anomalous_regions(dem_map[region], anomalous_regions_width, anomalous_pct)\n",
    "        \n",
    "        print('Saving pickle /Users/truggles/tmp_data{}/pickle_{}{}_r2.pkl'.format(app, region, version))\n",
    "        pickle_file = open('/Users/truggles/tmp_data{}/pickle_{}{}_r2.pkl'.format(app, region, version), 'wb') \n",
    "        pickle.dump(dem_map[region], pickle_file)\n",
    "        pickle_file.close()\n",
    "        dem_map[region].to_csv('/Users/truggles/tmp_dataOct7/csv_{}.csv'.format(region))\n",
    "        #continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    if load_from_pickle:\n",
    "        print('Loading from pickle /Users/truggles/tmp_data{}/pickle_{}{}_r2.pkl'.format(app, region, version))\n",
    "        pickle_in = open('/Users/truggles/tmp_data{}/pickle_{}{}_r2.pkl'.format(app, region, version),'rb')\n",
    "        dem_map[region] = pickle.load(pickle_in)\n",
    "        dem_map[region]['date_time'] = pd.to_datetime(dem_map[region]['date_time'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    ax.plot(dem_map[region]['demand (MW)'], 'k-', label='demand')\n",
    "    ax.plot(dem_map[region]['globalDemandFiltered'], 'g-', label='globalDemandFiltered')\n",
    "    ax.plot(dem_map[region]['localDemandFilteredUp'], 'r-', label='localDemandFiltered')\n",
    "    ax.plot(dem_map[region]['localDemandFilteredDown'], 'r-', label='_nolegend_')\n",
    "    ax.plot(dem_map[region]['deltaFiltered'], 'b-', label='demandFiltered')\n",
    "    ax.plot(dem_map[region]['runFiltered'], 'y-', label='runFiltered')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.title(\"{} Cleaned Demand\".format(region))\n",
    "    plt.savefig('plt/{}_demand_show_filters.png'.format(region))\n",
    "    \n",
    "    width = 240\n",
    "    title = '{} Demand Showing Filters'.format(region)\n",
    "    \n",
    "    save = '{}{}_demand_show_filters.png'.format(png_path, region)\n",
    "\n",
    "    targets = [95, 96, 101, 27]\n",
    "    targets = [-1,]\n",
    "    plus = 200\n",
    "    scrolling_demand(width, region, dem_map[region], title, save, local_dem_cut_up, local_dem_cut_down,\n",
    "                    delta_multiplier, delta_multiplier, targets, plus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_final_output = True\n",
    "version = '_v12_2day'\n",
    "#version = '_v12_2day_VandV_13Sept'\n",
    "print(\"prep_final_output {}\".format(prep_final_output))\n",
    "\n",
    "regions = return_all_regions()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regions.sort()\n",
    "print(regions)\n",
    "for i, region in enumerate(regions):\n",
    "    if not prep_final_output:\n",
    "        break\n",
    "    if region in ['SEC', 'OVEC']:\n",
    "        print(\"REGION SKIPPED\")\n",
    "        continue\n",
    "    print('Loading from pickle /Users/truggles/tmp_data2/pickle_{}{}_r2.pkl'.format(region, version))\n",
    "    pickle_in = open('/Users/truggles/tmp_data2/pickle_{}{}_r2.pkl'.format(region, version),'rb')\n",
    "    if i == 0: # Load first instance to master\n",
    "        master = pickle.load(pickle_in)\n",
    "        master['date_time'] = pd.to_datetime(master['date_time'])\n",
    "        master[region] = master['demand (MW)']\n",
    "        master[region+'_category'] = master['category']\n",
    "        master = master.drop(['demand (MW)', 'category', 'missing', 'runFiltered', \n",
    "                              'globalDemandFiltered', 'globalDemPlusMinusFiltered',\n",
    "                              'rollingDem', 'deltaFiltered',\n",
    "                              'deltaSingleFilteredFwd', 'deltaSingleFilteredBkw',\n",
    "                              'rollingDemLong', 'dem_minus_rolling', \n",
    "                              'vals_dem_minus_rolling', 'localDemandFilteredUp', 'localDemandFilteredDown',\n",
    "                              'hourly_median_dem_dev', 'dem_minus_rolling_IQR',\n",
    "                              'delta_pre', 'delta_post', 'delta_rolling_IQR',\n",
    "                              'dem_rel_diff_wrt_hourly_delta_post',\n",
    "                              'dem_rel_diff_wrt_hourly_delta_pre',\n",
    "                              'dem_rel_diff_wrt_hourly_long', 'dem_rel_diff_wrt_hourly',\n",
    "                              'len_good_data', 'anomalousRegionsFiltered'\n",
    "                             ], axis=1)\n",
    "        continue\n",
    "        \n",
    "\n",
    "    df = pickle.load(pickle_in)\n",
    "    master[region] = df['demand (MW)']\n",
    "    master[region+'_category'] = df['category']\n",
    "\n",
    "if prep_final_output:\n",
    "    print(master.head(5))\n",
    "    master.to_csv('/Users/truggles/tmp_data4/csv_MASTER_XXX{}.csv'.format(version), index=False, na_rep='NA')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### THIS IS FOR CHECKING IF IMPUTATIONS PASS THE ORIGINAL SELECTIONS ###\n",
    "\n",
    "do_check = True\n",
    "if not do_check:\n",
    "    assert(2+2==5)\n",
    "\n",
    "\n",
    "global_dem_cut = 10\n",
    "local_dem_cut_up = 3.5\n",
    "local_dem_cut_down = 2.5\n",
    "delta_multiplier = 2\n",
    "delta_single_multiplier = 5\n",
    "rel_multiplier = 15\n",
    "anomalous_regions_width = 24\n",
    "anomalous_pct = .85\n",
    "\n",
    "\n",
    "\n",
    "dump_to_pickle = False\n",
    "second_filter_and_save = False\n",
    "load_from_pickle = True\n",
    "version = '_v12_2day_VandV_13Sept'\n",
    "old_version = '_v12_2day'\n",
    "\n",
    "\n",
    "regions = return_all_regions()\n",
    "regions.sort()\n",
    "\n",
    "\n",
    "\n",
    "master_file = pd.read_csv('~/Downloads/mean_impute_csv_MASTER_v12_2day_mice_Sept13.csv')\n",
    "master_file['date_time'] = pd.to_datetime(master_file['date_time'])\n",
    "\n",
    "regions = ['FPC','JEA','WALC','SPA','NWMT','IID']\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    if region in ['SEC', 'OVEC']:\n",
    "        print(\"REGION SKIPPED\")\n",
    "        continue\n",
    "\n",
    "        \n",
    "    ### Grab the ORIGINAL version of this file\n",
    "    #\n",
    "    #   This allows us to get the original IQR thresholds\n",
    "    #   for comparing deltas most accurately\n",
    "    #\n",
    "    ###\n",
    "    file_path = '/Users/truggles/tmp_dataOct15/pickle_{}{}_r2.pkl'.format(region, old_version)\n",
    "    print('Loading ORIGINAL pickle {}'.format(file_path))\n",
    "    pickle_in = open(file_path,'rb')\n",
    "    old_df = pickle.load(pickle_in)\n",
    "        \n",
    "        \n",
    "        \n",
    "    dem_map = {}\n",
    "    if dump_to_pickle:\n",
    "        dem_map[region] = master_file[['date_time', region]].copy()\n",
    "        dem_map[region]['demand (MW)'] = dem_map[region][region]\n",
    "        \n",
    "        # Add categories to track filtering\n",
    "        dem_map[region] = add_categories(dem_map[region])\n",
    "        \n",
    "        # Missing and empty values are marked\n",
    "        dem_map[region] = dem_map[region].assign(missing=dem_map[region]['demand (MW)'].isna())\n",
    "\n",
    "        # Set all negative and zero values to NAN\n",
    "        dem_map[region] = filter_neg_and_zeros(dem_map[region])\n",
    "        \n",
    "        # Set last demand values in runs of 3+ to NAN\n",
    "        dem_map[region] = filter_runs(dem_map[region])\n",
    "\n",
    "        # Global demand filter on 10x the median value\n",
    "        dem_map[region] = filter_extrem_demand(dem_map[region], global_dem_cut)\n",
    "        \n",
    "        # Filter +/- 1 hour from any global deman filtered hours\n",
    "        dem_map[region] = filter_global_plus_minus_one(dem_map[region])\n",
    "        \n",
    "\n",
    "        # Add rolling dem average\n",
    "        dem_map[region] = add_rolling_dem(dem_map[region])\n",
    "        dem_map[region] = add_rolling_dem_long(dem_map[region])\n",
    "        dem_map[region] = add_demand_minus_rolling_dem(dem_map[region])\n",
    "        dem_map[region] = add_hourly_median_dem_deviations(dem_map[region])\n",
    "        dem_map[region] = add_demand_minus_rolling_dem_iqr(dem_map[region])\n",
    "        #FIXME??? dem_map[region]['dem_minus_rolling_IQR'] = old_df['dem_minus_rolling_IQR']\n",
    "        \n",
    "\n",
    "        # Add deltas\n",
    "        dem_map[region] = add_deltas(dem_map[region])\n",
    "        dem_map[region] = add_rolling_delta_iqr(dem_map[region])\n",
    "        dem_map[region]['delta_rolling_IQR'] = np.where(\n",
    "            ((old_df['delta_rolling_IQR'] < dem_map[region]['delta_rolling_IQR']) | \\\n",
    "            old_df['delta_rolling_IQR'].isna()), dem_map[region]['delta_rolling_IQR'],\n",
    "            old_df['delta_rolling_IQR'])\n",
    "    \n",
    "\n",
    "        #dem_map[region] = filter_local_demand(dem_map[region], local_dem_cut_up, local_dem_cut_down)\n",
    "        #dem_map[region] = filter_deltas(dem_map[region], delta_multiplier)\n",
    "        \n",
    "        print('Saving pickle /Users/truggles/tmp_data4/pickle_{}{}.pkl'.format(region, version))\n",
    "        pickle_file = open('/Users/truggles/tmp_data4/pickle_{}{}.pkl'.format(region, version), 'wb') \n",
    "        pickle.dump(dem_map[region], pickle_file)\n",
    "        pickle_file.close()\n",
    "        #dem_map[region].to_csv('/Users/truggles/tmp_data2/csv_{}.csv'.format(region))\n",
    "        #continue\n",
    "\n",
    "        \n",
    "    if second_filter_and_save:\n",
    "        print('Loading from pickle /Users/truggles/tmp_data4/pickle_{}{}.pkl'.format(region, version))\n",
    "        pickle_in = open('/Users/truggles/tmp_data4/pickle_{}{}.pkl'.format(region, version),'rb')\n",
    "        dem_map[region] = pickle.load(pickle_in)\n",
    "        dem_map[region]['date_time'] = pd.to_datetime(dem_map[region]['date_time'])\n",
    "        \n",
    "        dem_map[region] = filter_local_demand(dem_map[region], local_dem_cut_up, local_dem_cut_down)\n",
    "        dem_map[region] = filter_deltas(dem_map[region], delta_multiplier)\n",
    "        dem_map[region] = add_demand_rel_diff_wrt_hourly(dem_map[region])\n",
    "        dem_map[region] = filter_deltas_marching(dem_map[region], delta_single_multiplier, rel_multiplier)\n",
    "        \n",
    "        dem_map[region] = filter_anomalous_regions(dem_map[region], anomalous_regions_width, anomalous_pct)\n",
    "        \n",
    "        print('Saving pickle /Users/truggles/tmp_data4/pickle_{}{}_r2.pkl'.format(region, version))\n",
    "        pickle_file = open('/Users/truggles/tmp_data4/pickle_{}{}_r2.pkl'.format(region, version), 'wb') \n",
    "        pickle.dump(dem_map[region], pickle_file)\n",
    "        pickle_file.close()\n",
    "        #dem_map[region].to_csv('/Users/truggles/tmp_data4/csv_{}.csv'.format(region))\n",
    "        #continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    if load_from_pickle:\n",
    "        print('Loading from pickle /Users/truggles/tmp_data4/pickle_{}{}_r2.pkl'.format(region, version))\n",
    "        pickle_in = open('/Users/truggles/tmp_data4/pickle_{}{}_r2.pkl'.format(region, version),'rb')\n",
    "        dem_map[region] = pickle.load(pickle_in)\n",
    "        dem_map[region]['date_time'] = pd.to_datetime(dem_map[region]['date_time'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    ax.plot(dem_map[region]['demand (MW)'], 'k-', label='demand')\n",
    "    ax.plot(dem_map[region]['globalDemandFiltered'], 'g-', label='globalDemandFiltered')\n",
    "    ax.plot(dem_map[region]['localDemandFilteredUp'], 'r-', label='localDemandFiltered')\n",
    "    ax.plot(dem_map[region]['localDemandFilteredDown'], 'r-', label='_nolegend_')\n",
    "    ax.plot(dem_map[region]['deltaFiltered'], 'b-', label='demandFiltered')\n",
    "    ax.plot(dem_map[region]['runFiltered'], 'y-', label='runFiltered')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.title(\"{} Cleaned Demand\".format(region))\n",
    "    plt.savefig('plt/{}_demand_show_filters.png'.format(region))\n",
    "    \n",
    "    width = 240\n",
    "    title = '{} Demand Showing Filters'.format(region)\n",
    "    save = '/Users/truggles/tmp_plots{}xxx/{}_demand_show_filters.png'.format(version, region)\n",
    "\n",
    "    targets = [95, 96, 101, 27]\n",
    "    targets = [-1,]\n",
    "    plus = 200\n",
    "    scrolling_demand(width, region, dem_map[region], title, save, local_dem_cut_up, local_dem_cut_down,\n",
    "                    delta_multiplier, delta_multiplier, targets, plus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['CONUS_from_BAs_for_MEM', 'EASTERN_from_BAs_for_MEM', \n",
    "           'TEXAS_from_BAs_for_MEM', 'WESTERN_from_BAs_for_MEM']\n",
    "dfs = {}\n",
    "for r in regions:\n",
    "    dfs[r] = pd.read_csv('~/data5_out/{}.csv'.format(r))\n",
    "\n",
    "for r in regions:\n",
    "    print(r)\n",
    "    print(dfs[r].head())\n",
    "    print(dfs[r].isna().sum())\n",
    "\n",
    "print((dfs['CONUS_from_BAs_for_MEM']['demand (MW)'] == dfs['EASTERN_from_BAs_for_MEM']['demand (MW)'] + \\\n",
    "           dfs['TEXAS_from_BAs_for_MEM']['demand (MW)'] + dfs['WESTERN_from_BAs_for_MEM']['demand (MW)']).sum())\n",
    "print(len(dfs['CONUS_from_BAs_for_MEM'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
