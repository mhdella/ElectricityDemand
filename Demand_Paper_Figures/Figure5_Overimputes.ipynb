{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Demand Imputation Paper\n",
    "\n",
    "The most final overimputation files provided by Dave F. are in `Overimpute_Jan_2020_All_Files.zip` in my google drive `My Drive/Demand_Imputation/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.imputation.mice as smi\n",
    "import copy\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "!conda list matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with various verbose summaries\n",
    "def load_csv(file_path, columns, na_vals=['NA',], parse_d=True, verbose=0):\n",
    "    dtype_map = {}\n",
    "    for col in columns:\n",
    "        dtype_map[col] = np.float64\n",
    "    df = pd.read_csv(file_path,\n",
    "                    dtype=dtype_map,\n",
    "                    parse_dates=parse_d,\n",
    "                    na_values=na_vals)\n",
    "    if verbose >= 1:\n",
    "        #print(\"\\nHead(10)\")\n",
    "        #print(df.head(10))\n",
    "        print(\"\\nDescribe\")\n",
    "        print(df.describe().round(2))\n",
    "        print(\"\\ndf.isna().sum(axis=0)\")\n",
    "        print(df.isna().sum(axis=0))\n",
    "    if verbose >= 2:\n",
    "        plt.imshow(~df.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.show()\n",
    "    if 'date_time' in df.columns:\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lags(df, regions):\n",
    "    for r in regions:\n",
    "        print(\"Adding lag for region {}\".format(r))\n",
    "        kwargs = {\n",
    "                 '{}_Lag1'.format(r) : lambda x: np.roll(df[r], +1),\n",
    "             }\n",
    "        df = df.assign(**kwargs)\n",
    "    return df\n",
    "\n",
    "def drop_col(df, col):\n",
    "    return df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "def return_imputed_indices(raw, name):\n",
    "    index = pd.isnull(raw[name]).nonzero()[0]\n",
    "    return index\n",
    "\n",
    "def get_overimpute_index(raw, imp, col):\n",
    "    init_nan = return_imputed_indices(raw, col)\n",
    "    init_nan_set = set()\n",
    "    for i in init_nan:\n",
    "        init_nan_set.add(i)\n",
    "    \n",
    "    over_nan = return_imputed_indices(imp, col)\n",
    "    over_nan_set = set()\n",
    "    for i in over_nan:\n",
    "        over_nan_set.add(i)\n",
    "    \n",
    "    return np.array(list(over_nan_set.difference(init_nan_set)))\n",
    "\n",
    "def split_index_into_sort_and_long_gaps(index):\n",
    "    short = []\n",
    "    long = []\n",
    "    index.sort()\n",
    "    prev_was_short = False\n",
    "    for i in range(len(index)-2): # Can't compare the last one like this\n",
    "        if index[i+1] == index[i] + 1 and index[i+2] == index[i] + 2:\n",
    "            long.append(index[i])\n",
    "            prev_was_short = False\n",
    "        else:\n",
    "            short.append(index[i])\n",
    "            prev_was_short = True\n",
    "    if prev_was_short:\n",
    "        short.append(index[-2])\n",
    "        short.append(index[-1])\n",
    "    else:\n",
    "        long.append(index[-2])\n",
    "        long.append(index[-1])\n",
    "    return short, long\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "def return_values_by_index(imp, indices, name, replace_nan_with_zero=True):\n",
    "    vals = imp.loc[indices, name]\n",
    "    if replace_nan_with_zero:\n",
    "        vals = vals.fillna(0)\n",
    "    if len(vals) == 0:\n",
    "        print(\"return_values_by_index returned ZERO values\")\n",
    "    return vals\n",
    "\n",
    "def comparison_demand_plot(region, original, imp, imp_up, iqr_high, iqr_low, imp_down, imp_name, title, save, o_max):\n",
    "    \n",
    "    plt.close()\n",
    "    matplotlib.rcParams.update({'font.size': 22})\n",
    "    fig, ax = plt.subplots(figsize=(15,9))\n",
    "    plt.subplots_adjust(left=.15, bottom=0.25, top=0.97, right=0.95)\n",
    "    ax.set_ylabel('Demand (MW)')\n",
    "    ax.yaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    #plt.title(title)\n",
    "    ax.plot(original['date_time'], original[region], 'k-', label='Demand', linewidth=3.0)\n",
    "    ax.plot(original['date_time'], imp, 'r-', label='Imputed mean demand', linewidth=2.0)\n",
    "    ax.fill_between(original['date_time'], imp_down, imp_up, facecolor='orange', alpha=0.6, label='Full imputed range')\n",
    "    ax.fill_between(original['date_time'], iqr_low, iqr_high, facecolor='blue', label='IQR of imputed range')\n",
    "    ax.plot(original['date_time'], original[region], 'k-', label='_nolegend_', linewidth=3.0)\n",
    "    #ax.set_ylim(0, o_max*1.3)\n",
    "    ax.set_ylim(0, ax.get_ylim()[1]*1.3)\n",
    "    plt.legend(prop={'size': 24})  \n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%a\\n%Y-%m-%d'))\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=90 )\n",
    "    #ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    ax.set_ylim(min(ax.get_ylim()[0], 0), ax.get_ylim()[1])\n",
    "    #plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save+'.pdf')\n",
    "\n",
    "# Create many demand plots so we can actually see the values\n",
    "def scrolling_demand(width, region, original, imps, imp_mean, imp_name, title, save, tgt_itr=-1):\n",
    "    start = 0\n",
    "    end = width-1\n",
    "    if region == 'CISO':\n",
    "        start += 120\n",
    "        end += 120\n",
    "    k = 0\n",
    "    tot_l = len(original.index)\n",
    "    o_max = np.nanmax(original[region])\n",
    "    while True:\n",
    "        print(k, start, end)\n",
    "        s = save.replace('.pdf', '_{}cnt'.format(k))\n",
    "        t = title+': cnt {}'.format(k)\n",
    "        o = original.loc[start:end]\n",
    "        imp_avg = imp_mean.loc[start:end]\n",
    "        \n",
    "        \n",
    "        # Max and min\n",
    "        imp_max = []\n",
    "        iqr_high = []\n",
    "        iqr_low = []\n",
    "        imp_min = []\n",
    "        end_l = end if end == len(original.index) else end+1\n",
    "        \n",
    "        if tgt_itr == -1 or k == tgt_itr:\n",
    "            for j in range(start, end_l):\n",
    "            \n",
    "                #print(j)\n",
    "                vals = []\n",
    "            \n",
    "                for cnt, imp in enumerate(imps):\n",
    "                    #print(f\"{j} {cnt} --- max {max_}: min {min_}\")\n",
    "                    vals.append(imp.iloc[j][region])\n",
    "                \n",
    "                iqr_high.append(np.percentile(vals, 75))\n",
    "                iqr_low.append(np.percentile(vals, 25))\n",
    "                imp_max.append(max(vals))\n",
    "                imp_min.append(min(vals))\n",
    "\n",
    "                \n",
    "        \n",
    "            # Don't waste a plot if no imputation happened\n",
    "            if not (o[region].equals(imp_avg[region])):\n",
    "                print(f\"Actually plotting for hours: {start}, {end}\")\n",
    "                comparison_demand_plot(region, o, imp_avg[region], imp_max, iqr_high, iqr_low, imp_min, imp_name, t, s, o_max)\n",
    "        if end == tot_l:\n",
    "            break\n",
    "        k += 1\n",
    "        start += width\n",
    "        end += width\n",
    "        if end >= tot_l:\n",
    "            end = tot_l\n",
    "        \n",
    "        # Speed this up\n",
    "        if region == 'CISO' and k > 105: break\n",
    "        if region == 'LDWP' and k > 7: break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comparison_scatter_plot(v1s, v2s, labels, t1, t2, title, save, float_y_min=False):\n",
    "\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_xlabel(t1)\n",
    "    ax.set_ylabel(t2)\n",
    "    max_v1s = np.max( list(map(lambda x: np.max(x), v1s)))\n",
    "    max_v2s = np.max( list(map(lambda x: np.max(x), v2s)))\n",
    "    ax.set_xlim(0, max_v1s*1.1)\n",
    "    min_v2s = 0\n",
    "    if float_y_min:\n",
    "        min_v2s = np.min( list(map(lambda x: np.min(x), v2s)))\n",
    "    ax.set_ylim(min_v2s, max_v2s*1.1)\n",
    "    plt.title(title)\n",
    "    for v1, v2, l in zip(v1s, v2s, labels):\n",
    "        ax.plot(v1, v2, '.', label=l, alpha=0.2)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "def simple_resolution(df_true, df_imp, title, save, n_bins=20):\n",
    "\n",
    "    plt.close()\n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting simple_resolution\")\n",
    "    #    return 0\n",
    "\n",
    "    #res_grid = []\n",
    "    #for index, value in df_true.items():\n",
    "    #    if value > 0:\n",
    "    #        res_grid.append( (df_imp.at[index]-value) / value)\n",
    "    #    else:\n",
    "    #        print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    res_grid = []\n",
    "    for obs, val in zip(df_true, df_imp):\n",
    "        if obs != 0:\n",
    "            res_grid.append((val-obs)/obs)\n",
    "        else:\n",
    "            print(\"Value == 0 for simple_resolution {} {}\".format(title, save))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    n, bins, patches = ax.hist(res_grid, n_bins, \n",
    "            facecolor='b', alpha=0.5, density=False)\n",
    "    print(\"Length simp_res {}\".format(np.sum(n)))\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('(Imp. - Obs.)/Obs.')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(save)\n",
    "\n",
    "\n",
    "def bplot_ylim(ax):\n",
    "    current_ylim = ax.get_ylim()\n",
    "    if current_ylim[0] > -0.1 and current_ylim[1] < 0.1:\n",
    "        ax.set_ylim(-0.1, 0.1)\n",
    "    elif current_ylim[0] > -0.15 and current_ylim[1] < 0.15:\n",
    "        ax.set_ylim(-0.15, 0.15)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def get_mape_list(df_master, df_true, df_imp):\n",
    "    mape_list = []\n",
    "    mean_list = []\n",
    "    \n",
    "    for obs, val in zip(df_true, df_imp):\n",
    "        res = (val-obs)/obs\n",
    "        mape_list.append( abs(res) )\n",
    "        mean_list.append( res )\n",
    "    \n",
    "    return mape_list, mean_list\n",
    "\n",
    "        \n",
    "# df_master is used to get the correct UCT time\n",
    "def resolution_by_time(df_master, idx_vals, df_true, df_imp, title, save):\n",
    "\n",
    "    plt.close()\n",
    "    matplotlib.rcParams.update({'font.size': 18})\n",
    "    \n",
    "    #if df_true.index.all() != df_imp.index.all():\n",
    "    #    print(\"Indices do not align, exiting resolution_by_time\")\n",
    "    #    return 0\n",
    "    \n",
    "    # Get UCT time by index\n",
    "    zero_index_hour = df_master.at[0, 'date_time'].hour\n",
    "    \n",
    "    res_grid = []\n",
    "    for i in range(24):\n",
    "        res_grid.append([])\n",
    "    month_grid = []\n",
    "    for i in range(12):\n",
    "        month_grid.append([])\n",
    "    week_grid = []\n",
    "    for i in range(7):\n",
    "        week_grid.append([])\n",
    "    \n",
    "    #print(idx_vals, df_true, df_imp)\n",
    "    for idx, obs, val in zip(idx_vals, df_true, df_imp):\n",
    "        #print(idx, obs, val)\n",
    "        mod = (idx + zero_index_hour)%24 # Do UTC (Nov 5, 2019) - 8)%24 # -8 for PST vs. UCT\n",
    "        res = (val-obs)/obs\n",
    "        #print(res)\n",
    "        #print(res_grid, mod, res)\n",
    "        res_grid[mod].append(res)\n",
    "        # Get month\n",
    "        month = df_master.at[idx, 'date_time'].month\n",
    "        month_grid[month-1].append(res)\n",
    "        # Get week\n",
    "        day = df_master.at[idx, 'date_time'].weekday()\n",
    "        week_grid[day].append(res)\n",
    "\n",
    "    medianprops = dict(linestyle='-', linewidth=2.5)\n",
    "    matplotlib.rcParams.update({'font.size': 22})\n",
    "    rot = 90\n",
    "    x_lab = '(imputed - actual)/actual'\n",
    "    # Plot hourly\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    plt.subplots_adjust(left=.15, bottom=0.2, top=0.97, right=0.95)\n",
    "    ax.yaxis.grid(True)\n",
    "    #ax.set_title(title+': whiskers at 5%/95%')\n",
    "    bplot = ax.boxplot(res_grid, whis=[5, 95], showfliers=False, patch_artist=True, medianprops=medianprops)\n",
    "    hour_labels = []\n",
    "    for i in range(24):\n",
    "        if i%2==0:\n",
    "            hour_labels.append('{0:02d}:00'.format(i))\n",
    "        else:\n",
    "            hour_labels.append('')\n",
    "    plt.xticks([i for i in range(1, 25)], hour_labels, rotation=rot)\n",
    "    ax.set_xlabel('Hour (UTC)')\n",
    "    ax.set_ylabel(x_lab)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "    #plt.tight_layout()\n",
    "    for patch in bplot['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    bplot_ylim(ax)\n",
    "    plt.savefig(save)\n",
    "    \n",
    "    # Plot monthly\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    plt.subplots_adjust(left=.15, bottom=0.2, top=0.97, right=0.95)\n",
    "    ax.yaxis.grid(True)\n",
    "    #ax.set_title(title.replace('hour', 'month')+': whiskers at 5%/95%')\n",
    "    bplot = ax.boxplot(month_grid, whis=[5, 95], showfliers=False, patch_artist=True, medianprops=medianprops)\n",
    "    plt.xticks([i for i in range(1, 13)], ('Jan','Feb','Mar','Apr','May','Jun',\n",
    "                                          'Jul','Aug','Sep','Oct','Nov','Dec'), rotation=rot)\n",
    "    ax.set_ylabel(x_lab)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "    #plt.tight_layout()\n",
    "    for patch in bplot['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    bplot_ylim(ax)\n",
    "    plt.savefig(save.replace('hour', 'month'))\n",
    "    \n",
    "    # Plot weekly\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    plt.subplots_adjust(left=.15, bottom=0.2, top=0.97, right=0.95)\n",
    "    ax.yaxis.grid(True)\n",
    "    #ax.set_title(title.replace('hour', 'week')+': whiskers at 5%/95%')\n",
    "    bplot = ax.boxplot(week_grid, whis=[5, 95], showfliers=False, patch_artist=True, medianprops=medianprops)\n",
    "    plt.xticks([i for i in range(1, 8)], ('Mon','Tue','Wed','Thu','Fri','Sat','Sun'), rotation=rot)\n",
    "    ax.set_ylabel(x_lab)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "    #plt.tight_layout()\n",
    "    for patch in bplot['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    bplot_ylim(ax)\n",
    "    plt.savefig(save.replace('hour', 'week'))\n",
    "\n",
    "    \n",
    "    \n",
    "def return_all_regions():\n",
    "    return ['AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "    'DUK', 'FMPP', 'FPC',\n",
    "    'FPL', 'GVL', 'HST', 'ISNE',\n",
    "    'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "    'NYIS', 'OVEC', 'PJM', 'SC',\n",
    "    'SCEG', 'SEC', 'SOCO',\n",
    "    'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "    'TVA', 'ERCO',\n",
    "    'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "    'CHPD', 'CISO', 'DOPD',\n",
    "    'EPE', 'GCPD', 'IID',\n",
    "    'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "    'PACE', 'PACW', 'PGE', 'PNM',\n",
    "    'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "    'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "    'WALC', 'WAUW']\n",
    "    \n",
    "    \n",
    "def return_good_regions():\n",
    "    regions = return_all_regions()\n",
    "    regions.remove('SEC')\n",
    "    regions.remove('OVEC')\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 240\n",
    "\n",
    "n1 = 'MICE'\n",
    "## Open a saved csv and check contents\n",
    "base = '/Users/truggles/Downloads/overimpute_Jan_2020_All_Files/'\n",
    "png_out = '/Users/truggles/tmp_plots_imp_20200225_boxes/'\n",
    "\n",
    "### NOTE, if you want ALL regions use regions = return_all_regions()\n",
    "regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "regions = ['CISO',]\n",
    "\n",
    "do_box_plots = True\n",
    "if not do_box_plots:\n",
    "    print(\"\\nOnly producing the scrolling demand plots\\n\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if do_box_plots:\n",
    "    regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "    regions = ['CISO',]\n",
    "    df = load_csv(base+'csv_MASTER_v12_2day.csv', regions, ['NA',], True, 0)\n",
    "    df = df.drop([i for i in range(19)])\n",
    "    df = df.drop([i for i in range(35064+19,36547)])\n",
    "    df = df.reset_index()\n",
    "\n",
    "    \n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "    \n",
    "\n",
    "# Control if MAPE and boxplots are made with all chains vs. the mean file for 20 chains\n",
    "run_ALL_chains = False\n",
    "print(f\"Run ALL chains {run_ALL_chains}\")\n",
    "\n",
    "    \n",
    "regs = ['CISO',]#'NYIS','ERCO','PJM']\n",
    "#regs = ['SCEG',]\n",
    "#regs = return_good_regions()\n",
    "print( f\"Length of regions: {len(regs)}\" )\n",
    "#regs.remove('SCEG') # Not sure why this was necessary.  :-(\n",
    "print(regs)\n",
    "# Loop all regions\n",
    "mape_map = {}\n",
    "for r in regs:\n",
    "    print(r)\n",
    "    if not do_box_plots:\n",
    "        break\n",
    "    record = {\n",
    "        'idx_short' : [],\n",
    "        'idx_long' : [],\n",
    "        \n",
    "        'obs_short' : [],\n",
    "        'obs_long' : [],\n",
    "\n",
    "        'imp_short' : [],\n",
    "        'imp_long' : [],\n",
    "    }\n",
    "    # Loop all imputed files\n",
    "    all_mice = []\n",
    "    \n",
    "    mape_map[r] = {}\n",
    "    \n",
    "    for i in range(1,17):\n",
    "        #print(f'{r} - adding mean file {i}')\n",
    "        df_imp = load_csv(base+f'csv_MASTER_v12_2day_{i}.csv', regions, ['NA',], True, 0)\n",
    "        df_imp = df_imp.drop([i for i in range(19)])\n",
    "        df_imp = df_imp.drop([i for i in range(35064+19,36547)])\n",
    "        df_imp = df_imp.reset_index()\n",
    "\n",
    "        \n",
    "        # Find index ONLY from overimputation\n",
    "        over_index = get_overimpute_index(df, df_imp, r)\n",
    "\n",
    "        # Split results by short vs long imputation gaps\n",
    "        short_obs, long_obs = split_index_into_sort_and_long_gaps(over_index)\n",
    "        \n",
    "        \n",
    "        # For MEAN of 20 chains chains\n",
    "        if not run_ALL_chains:\n",
    "            df_mice = load_csv(base+f'mean_overimpute_csv_MASTER_v12_2day_{i}_mice_Jan16.csv', regions, ['NA',], True, 0)\n",
    "\n",
    "            \n",
    "            record['idx_short'].append(short_obs)\n",
    "            record['idx_long'].append(long_obs)\n",
    "        \n",
    "            record['obs_short'].append(return_values_by_index(df, short_obs, r))\n",
    "            record['obs_long'].append(return_values_by_index(df, long_obs, r))\n",
    "        \n",
    "            record['imp_short'].append(return_values_by_index(df_mice, short_obs, r))\n",
    "            record['imp_long'].append(return_values_by_index(df_mice, long_obs, r))\n",
    "        \n",
    "        if run_ALL_chains:\n",
    "            for chain in range(1, 21):\n",
    "            \n",
    "                df_mice = load_csv(base+f'imp_{i}_chain_{chain}.csv', regions, ['NA',], True, 0)\n",
    "            \n",
    "                record['idx_short'].append(short_obs)\n",
    "                record['idx_long'].append(long_obs)\n",
    "        \n",
    "                record['obs_short'].append(return_values_by_index(df, short_obs, r))\n",
    "                record['obs_long'].append(return_values_by_index(df, long_obs, r))\n",
    "        \n",
    "                record['imp_short'].append(return_values_by_index(df_mice, short_obs, r))\n",
    "                record['imp_long'].append(return_values_by_index(df_mice, long_obs, r))\n",
    "\n",
    "    \n",
    "    idxs = ['idx_short', 'idx_long']\n",
    "    obss = ['obs_short', 'obs_long']\n",
    "    imps = ['imp_short', 'imp_long']\n",
    "    names = ['short', 'long']\n",
    "    for name, idx, obs, imp in zip(names, idxs, obss, imps):\n",
    "        idx_vals = np.concatenate(record[idx])\n",
    "        obs_vals = pd.concat(record[obs])\n",
    "        imp_vals = pd.concat(record[imp])\n",
    "\n",
    "\n",
    "        resolution_by_time(df, idx_vals, obs_vals, imp_vals, 'overimp hourly resolution {} {} {}'.format(name, n1, r),\n",
    "            f'{png_out}imp_resolution_hourly_{r}_{name}.pdf')\n",
    "        \n",
    "        info = get_mape_list(df, obs_vals, imp_vals)\n",
    "        mape_map[r][name] = info[0]\n",
    "        mape_map[r][name+'_mean'] = info[1]\n",
    "    mape_map[r]['all'] = list(mape_map[r]['short'])\n",
    "    mape_map[r]['all'].extend( list(mape_map[r]['long']) )\n",
    "    mape_map[r]['all_mean'] = list(mape_map[r]['short_mean'])\n",
    "    mape_map[r]['all_mean'].extend( list(mape_map[r]['long_mean']) )\n",
    "\n",
    "\n",
    "\n",
    "def get_forecast(reg):\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        f'/Users/truggles/EIA_Cleaned_Hourly_Electricity_Demand_Data/data/release_2019_Oct/balancing_authorities/{reg}.csv',\n",
    "        na_values=['MISSING','EMPTY']\n",
    "    )\n",
    "    to_drop = []\n",
    "    for idx in df.index:\n",
    "        if df.loc[idx, 'category'] != 'OKAY' or np.isnan(df.loc[idx, 'forecast demand (MW)']):\n",
    "            to_drop.append(idx)\n",
    "    df = df.drop(to_drop, axis=0)\n",
    "    df['f'] = (df['raw demand (MW)'] - df['forecast demand (MW)']).abs() /df['raw demand (MW)']\n",
    "    return(np.mean(df['f']))\n",
    "    \n",
    "if do_box_plots:\n",
    "    print(\"BA Median_Demand Num_for_Imp Mean_Diff Short Long All Forecast\")\n",
    "    total = {'short': [], 'long': [], 'all': [], 'all_mean': []}\n",
    "    tot_imp = 0\n",
    "    tot_med = 0\n",
    "    for r, d in mape_map.items():\n",
    "        med = df[r].median()\n",
    "        short_ = 100. * sum(d['short'])/len(d['short']) if len(d['short']) > 0 else np.nan\n",
    "        long_ = 100. * sum(d['long'])/len(d['long']) if len(d['long']) > 0 else np.nan\n",
    "        all_ = 100. * sum(d['all'])/len(d['all']) if len(d['all']) > 0 else np.nan\n",
    "        mean_all = np.nanmean(d['all_mean']) if len(d['all_mean']) > 0 else np.nan\n",
    "        total['short'].extend(d['short'])\n",
    "        total['long'].extend(d['long'])\n",
    "        total['all'].extend(d['all'])\n",
    "        total['all_mean'].extend(d['all_mean'])\n",
    "        tot_imp += sum(df[r].isna())\n",
    "        tot_med += med\n",
    "        forecast = get_forecast(r)\n",
    "        print(r, int(med), sum(df[r].isna()), 100. * round(mean_all,5), round(short_,2), round(long_,2), round(all_,2), round(forecast*100,2))\n",
    "    print(\"CONUS\", int(tot_med), tot_imp, 100. * round(np.nanmean(total['all_mean']),5),\n",
    "        round(100. * sum(total['short'])/len(total['short']),2), \n",
    "        round(100. * sum(total['long'])/len(total['long']),2), \n",
    "        round(100. * sum(total['all'])/len(total['all']),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overimpute validation plots with scrolling demand\n",
    "### based on ca_for_overimpute_0.csv\n",
    "### Dave returned a single LONG csv with 20 version indices with imputed copies\n",
    "### Current files all link to those found in overimpute_20191004_v0.zip on gDrive\n",
    "\n",
    "no_scrolling = False\n",
    "width = 240\n",
    "\n",
    "\n",
    "base = '~/Downloads/overimpute_20191004_v0/'\n",
    "\n",
    "if not no_scrolling:\n",
    "    regions = ['BANC', 'CISO', 'LDWP', 'TIDC']\n",
    "    df = load_csv(base+'csv_MASTER_v12_2day.csv', regions, ['NA',], True, 2)\n",
    "    imp_mean = load_csv(base+'mean_overimpute_new_clim_lag_lead_csv_MASTER_v12_2day_0_mice.csv', regions, ['NA',], True, 0)\n",
    "    df_imp = load_csv(base+'csv_MASTER_v12_2day_0.csv', regions, ['NA',], True, 0)\n",
    "    df_mice_all = load_csv(base+'all_overimpute_new_clim_lag_lead_csv_MASTER_v12_2day_0_mice_INT.csv', regions, ['NA',], True, 0)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    imp_mean['date_time'] = pd.to_datetime(imp_mean['date_time'])\n",
    "    df_imp['date_time'] = pd.to_datetime(df_imp['date_time'])\n",
    "    df_mice_all['date_time'] = pd.to_datetime(df_mice_all['date_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = ['CISO','BANC','LDWP']\n",
    "regs = ['LDWP','BANC', 'CISO']\n",
    "#regs = ['NYIS','PJM']\n",
    "regs = ['CISO','LDWP',]\n",
    "#regs = ['LDWP',]\n",
    "# Loop all regions\n",
    "\n",
    "tgt_map = {\n",
    "    'CISO' : 101,\n",
    "    'LDWP' : 5,\n",
    "}\n",
    "\n",
    "for r in regs:\n",
    "    if no_scrolling:\n",
    "        continue\n",
    "    print(r)\n",
    "    \n",
    "    # Loop all imputed files\n",
    "    all_mice = []\n",
    "    for i in range(1, 21):\n",
    "        #print(i)\n",
    "        \n",
    "        df_mice = df_mice_all.loc[df_mice_all['imp_index'] == i]\n",
    "        #print(len(df_mice))\n",
    "        all_mice.append(df_mice)\n",
    "    \n",
    "    scrolling_demand(width, r, df, all_mice, imp_mean, 'MICE',\n",
    "            'Imputation of {}'.format(r), \n",
    "            '/Users/truggles/tmp_plots_imp_Nov05_Feb25/imp_dem_{}_comp.pdf'.format(r),\n",
    "            tgt_map[r]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_for_missing_structure(df):\n",
    "    print(\"scan_for_missing_structure\")\n",
    "    rec = {}\n",
    "    for col in df.columns:\n",
    "        if 'Lag1' in col or 'date_time' in col: continue\n",
    "        rec[col] = [0, []] # missing tally, and record\n",
    "    for index, row in df.iterrows():\n",
    "        if index%1000==0:\n",
    "            print(\" - scanning row {}\".format(index))\n",
    "        for col, info in rec.items():\n",
    "            if np.isnan(row[col]): # incriment missing tally\n",
    "                info[0] += 1\n",
    "            elif not np.isnan(row[col]) and info[0] > 0:\n",
    "                info[1].append(info[0])\n",
    "                info[0] = 0\n",
    "    #for k, v in rec.items():\n",
    "    #    print(k, v)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def remove_locations(df, requested_gaps, cnt=-1):\n",
    "    print(\"remove_locations cnt {}\".format(cnt))\n",
    "    # Begin with the longest requested gaps and work you way\n",
    "    # to smaller requested gaps\n",
    "    for col, info in requested_gaps.items():\n",
    "        info[1].sort()\n",
    "        info[1].reverse()\n",
    "        #print(col, info[1])\n",
    "        for length in info[1]:\n",
    "            # Try in requested column first the simple way, if that doesn't\n",
    "            # work, try the \"difficult\" way where some existing\n",
    "            # np.nans will be included in naned data\n",
    "            #if find_and_remove_location(df, col, length):\n",
    "            #    find_and_remove_difficult_location(df, col, length, cnt)\n",
    "            find_and_remove_location2(df, col, length)\n",
    "\n",
    "            \n",
    "\n",
    "# Loop over vals in the dataframe and find a continous region which\n",
    "# has a reasonably \"good\" data buffer around the requested length of data to remove.\n",
    "# Change the valse to np.nan\n",
    "def find_and_remove_location2(df, col, length, verbose=False):\n",
    "    # Start at a random index position to not bias the removals\n",
    "    # all towards the front\n",
    "    loc = int(np.random.uniform(0, len(df.index)-length-2))\n",
    "\n",
    "    while True:\n",
    "        # Check that we begin with a \"good\" data value\n",
    "        if not np.isnan(df.at[loc, col]):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Will remove length {} from location with {} NANs\".format(\n",
    "                    length, df.loc[loc:loc+length, col].isna().sum()))\n",
    "            df.loc[loc+1:loc+length, col] = np.nan\n",
    "            if verbose:\n",
    "                print(\"Removed length {} from location with {} NANs\".format(\n",
    "                    length, df.loc[loc:loc+length, col].isna().sum()))\n",
    "            return 0\n",
    "        loc += 1\n",
    "        \n",
    "        # Wrap to start of df before reaching the end to\n",
    "        # ensure that the gap will fit\n",
    "        if loc + length >= len(df.index):\n",
    "            loc = 0\n",
    "\n",
    "\n",
    "# Loop over vals in the dataframe and find a continous region which\n",
    "# has a reasonably \"good\" data buffer around the requested length of data to remove.\n",
    "# Change the valse to np.nan\n",
    "def find_and_remove_location(df, col, length, verbose=False):\n",
    "    # Start at a random index position to not bias the removals\n",
    "    # all towards the front\n",
    "    max_good_data = 0\n",
    "    loc = int(np.random.uniform(0, len(df.index)))\n",
    "    if loc == len(df.index):\n",
    "        loc -= 1\n",
    "    start_of_good_data = loc\n",
    "    length_of_good_data = 0\n",
    "    n_loops = 0\n",
    "    # How much good data on each side of the new gap?\n",
    "    if length <= 100:\n",
    "        tgt_length = 5 * length \n",
    "        buffer = 2 * length # 2x on each side\n",
    "    elif length <= 1000:\n",
    "        tgt_length = 3 * length \n",
    "        buffer = 1 * length # 1x on each side\n",
    "    else:\n",
    "        tgt_length = int(1.5 * length) \n",
    "        buffer = int(0.25 * length) # 0.25x on each side\n",
    "    while True:\n",
    "        if np.isnan(df.at[loc, col]):\n",
    "            start_of_good_data = loc + 1 # This is the following value\n",
    "            # and will continuously incriment if isnan()\n",
    "            length_of_good_data = 0\n",
    "        else: # good data\n",
    "            length_of_good_data += 1\n",
    "            if length_of_good_data > max_good_data:\n",
    "                max_good_data = length_of_good_data\n",
    "\n",
    "        # Remember pandas DataFrame has different slice notation that normal python\n",
    "        # where the terminal value is included in the slice\n",
    "        if length_of_good_data == tgt_length - 1:\n",
    "            if verbose:\n",
    "                print(\"Found a great spot for removal, col {:}, l={:d}, tgt_l={:d}, buffer={:d}, [{:d}:{:d}]\".format(\n",
    "                        col, length, tgt_length, buffer, start_of_good_data, start_of_good_data+length_of_good_data))\n",
    "            strt = start_of_good_data + buffer # Begin nan after good data buffer \n",
    "            end = start_of_good_data + buffer + length - 1\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            df.loc[strt:end, col] = np.nan\n",
    "            if verbose:\n",
    "                print(df.loc[start_of_good_data:start_of_good_data+length_of_good_data, col])\n",
    "            return 0\n",
    "        loc += 1\n",
    "        \n",
    "        # Wrap to start of df\n",
    "        if loc >= len(df.index):\n",
    "            loc = 0\n",
    "            n_loops += 1\n",
    "            if n_loops > 1:\n",
    "                print(\"Too many loops for col {} and requested length {}, max good data length {}\".format(\n",
    "                    col, length, max_good_data))\n",
    "                return 1\n",
    "\n",
    "            \n",
    "def find_and_remove_difficult_location(df, col, length, to_take=-1, verbose=False):\n",
    "    # Scan data and look for highest purity \"good\" data region\n",
    "    # to apply np.nan\n",
    "    rec = []\n",
    "    print(\"find_and_remove_difficult_location\")\n",
    "    print(\" - Looking for col {} for length {}\".format(col, length))\n",
    "    for i in range(0, int(len(df.index) - 1.5 * length)):\n",
    "        rec.append((i, df.loc[i:int(i+1.5*length), col].isna().sum()))\n",
    "    nan_min = 9999\n",
    "    best_idx = -1\n",
    "    for val in rec:\n",
    "        if val[1] < nan_min:\n",
    "            nan_min = val[1]\n",
    "            best_idx = val[0]\n",
    "\n",
    "    # Find other comparable locations for adding a gap.\n",
    "    # Search for reginons with 20% more less NANs\n",
    "    # with respect to length requested.\n",
    "    others = []\n",
    "    for val in rec:\n",
    "        if val[1] <= length * 0.2 and abs(val[0]-best_idx) > length:\n",
    "            new_gap = True\n",
    "            for other in others:\n",
    "                if abs(val[0]-other[0]) < length:\n",
    "                    new_gap = False\n",
    "            if new_gap and val[0] - length < len(df.index):\n",
    "                others.append(val)\n",
    "    print(\" - others:\")\n",
    "    print(others)\n",
    "    \n",
    "    \n",
    "    print(\" - Best idx {} for nan count of {}\".format(best_idx, nan_min))\n",
    "    if to_take >= 0:\n",
    "        print(others[to_take][0])\n",
    "        print(\" - Will select location based on 'Others' {} {}\".format(to_take, others[to_take]))\n",
    "        df.loc[int(others[to_take][0]+0.25*length):int(others[to_take][0]+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in 'other' location {} with {} total np.nans\".format(\n",
    "            others[to_take][0],\n",
    "            df.loc[others[to_take][0]:int(others[to_take][0]+1.5*length), col].isna().sum()))\n",
    "    else:\n",
    "        df.loc[int(best_idx+0.25*length):int(best_idx+1.25*length), col] = np.nan\n",
    "        print(\" - Difficult NAN insertion resulting in defaul location {} with {} total np.nans\".format(\n",
    "            best_idx,\n",
    "            df.loc[best_idx:int(best_idx+1.5*length), col].isna().sum()))\n",
    "            \n",
    "# Drop the category columns\n",
    "def drop_cols_for_overimpute(df):\n",
    "    cols = df.columns\n",
    "    for col in cols:\n",
    "        if '_category' in col:\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "version = 'v7_2day'\n",
    "path = '/Users/truggles/tmp_data/'\n",
    "file = 'csv_MASTER'\n",
    "file_path = path+file+'_{}.csv'.format(version)\n",
    "regions = return_all_regions()\n",
    "df = load_csv(file_path, regions, ['NA',], True, 0)\n",
    "df = drop_cols_for_overimpute(df)\n",
    "\n",
    "np.random.seed(1)\n",
    "plt.imshow(~df.isna(), aspect='auto')\n",
    "plt.xlabel(\"variables\")\n",
    "plt.ylabel(\"cases\")\n",
    "plt.gray()\n",
    "plt.savefig('{}_{}.png'.format(file, version))\n",
    "\n",
    "make_overimpute = False\n",
    "if make_overimpute:\n",
    "    results = scan_for_missing_structure(df)\n",
    "    for i in range(0, 2):\n",
    "        df2 = copy.deepcopy(df)\n",
    "        results2 = copy.deepcopy(results)\n",
    "        remove_locations(df2, results2, i)\n",
    "        plt.imshow(~df2.isna(), aspect='auto')\n",
    "        plt.xlabel(\"variables\")\n",
    "        plt.ylabel(\"cases\")\n",
    "        plt.gray()\n",
    "        plt.savefig('{}_{}_{}.png'.format(file, version, str(i)))\n",
    "        print(\"Saving as '{}_{}_{}.csv'\".format(file, version, str(i)))\n",
    "        df2.to_csv('{}_{}_{}.csv'.format(file, version, str(i)), index=False, na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
